# -*- coding: utf-8 -*-
"""Медиатека регрессия v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EjAaSr6-7UXS-_08wzOwyeeUCq3yiV6U

# ИНФО

Модель регрессии

Доработки:


1.   На остове колонки spotDay добавлены колонки spotMonth (месяц), weedDay (день недели), weekEnd(рабочий/выходной)
2.   Все пустые поля заполены '-'
3.   На остове квантилей по колонкам с показателяти звонков ограничен диапазон значений по верхней границе в районе 97%. Т.е. оставшимся 3% присвоены максимальные выбранные значения для этих данных.
4.   Задана архитектура НС по результатам экспериментов с регрессией Антона.
5.   Добавлена колонка genre_class на основе классификации Антона колонки genre.
6.   Добавлен словарь dataX_for_site_dict для отображения полей и их значений на сайте для ввода входных значений для единичного предсказания.

# Корневой каталог
"""

path_main = '/content/drive/MyDrive/КУРСЫ/УИИ/СТАЖИРОВКА 202209/НС/v3/'

"""# Импорт Библиотек"""

import numpy as np #Библиотека работы с массивами
import pandas as pd # Библиотека для работы с базами

from tensorflow.keras.models import Sequential, Model, load_model # 
from tensorflow.keras.layers import concatenate, Input, Dense, Dropout, BatchNormalization, Flatten #
from tensorflow.keras import utils #Используем для to_categoricall
from tensorflow.keras.optimizers import Adam,Adadelta,SGD,Adagrad,RMSprop #
from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence #
from tensorflow.keras.preprocessing.sequence import pad_sequences #
from tensorflow.keras.callbacks import LambdaCallback # подключаем колбэки

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split # Для разбивки на выборки
import joblib # для сохранения MinMaxScaler

import matplotlib.pyplot as plt #

import pickle # для записи словаря в файл в формате JSON

"""# Функции

## Сохранить|загрузить данные в файл
"""

def save_data(path, data):
  """
  Сохраняет словарь в файл
  path - путь к файлу
  data - сохраняемые данные (словарь, список и т.д.)

  Описание https://pythonworld.ru/moduli/modul-pickle.html
  """

  with open(path, 'wb') as f:
    pickle.dump(data, f)

  
def load_data(path):
  """
  Загружает словарь из файла
  path - путь к файлу

  Возвращает:
  data - загруженные данные (словарь, список и т.д.)

  Описание https://pythonworld.ru/moduli/modul-pickle.html
  """

  with open(path, 'rb') as f:
    return pickle.load(f)

"""## Сохранить|загрузить словарь"""

def save_dict(path, data_dict):
  """
  Сохраняет словарь в файл
  path - путь к файлу
  data_dict - словарь
  """

  save_data(path, data_dict)

  
def load_dict(path):
  """
  Загружает словарь из файла
  path - путь к файлу

  Возвращает:
  data_dict - словарь
  """

  data_dict = load_data(path)

  return data_dict

"""## Сохранить|загрузить маску"""

def save_masks(path, mask_train, mask_test):
  """
  Сохраняет словарь в файл
  path - путь к файлу
  mask_train - маска тренировочная
  mask_test - маска проверочная
  """
  mask = {'mask_train' : mask_train,
          'mask_test' : mask_test}

  save_data(path, mask)

  
def load_masks(path):
  """
  Загружает словарь из файла
  path - путь к файлу

  Возвращает:
  mask_train - маска тренировочная
  mask_test - маска проверочная
  """

  mask = load_data(path)

  return mask['mask_train'], mask['mask_test']

"""# Подготовка БД"""

from google.colab import drive
drive.mount('/content/drive')

"""##Загружаем базу и смотрим содержание"""

df_db = pd.read_csv(path_main + 'db_csv.csv', sep=',')
df_db.head(5)

"""##Оставляем только нужное из базы"""

# Имена колонок с входными данными
#columnsX = ['advertiser', 'product', 'TVChannel', 'spread', 'spotLine', 'spotDay', 'spotDuration']
columnsX = ['advertiser', 'product', 'TVChannel', 'spread', 'genre', 'spotLine', 'spotDay', 'spotDuration']
# Имена колонок с выходными данными
columnsY = ['AllCalls', 'calls(0)', 'calls(-2)', 'calls(-4)', 'calls(-7)']

columns = columnsX + columnsY
df_data = pd.DataFrame(data=df_db[columns], index=None, columns=columns, dtype=None)
df_data['spotDay'] = pd.to_datetime(df_data['spotDay'], errors='coerce') # задаём тип столбцу spotDay

# колонку spotDuration переводим в тип object
df_data['spotDuration'] = df_data['spotDuration'].astype(str)

# Перевод spotLine в формат 00:00:00, чтобы удобно было сортировать
df_data['spotLine'] = pd.to_datetime(df_data['spotLine']) # задаём тип столбцу spotLine
df_data['spotLine'] = df_data['spotLine'].dt.strftime('%H:%M:%S')
print(df_data.dtypes)
df_data

"""## Подчищаем базу от NaN"""

# все пустые (не заполненные) ячейки в колонках типа object меняем на '-'
df_data.fillna(
    value = {'advertiser': '-',
             'product': '-',
             'TVChannel': '-',
             'spread': '-',
             'genre': '-',
             'spotLine': '-',
             'spotDuration': '-'}, 
    inplace=True)

# Проверяем на пустые значения
df_data.isna().sum()

"""## Расширяем базу"""

# Словарь типов тв-программ
genre_map = { 0 : '-',
              1 : 'Кино, сериалы и анимация',
              2 : 'Люди и общество (шоу)',
              3 : 'Документальная программа, сериал',
              4 : 'Новости и политика',
              5 : 'Развлечения',
              6 : 'Игры',
              7 : 'Образование, познавательная программа',
              8 : 'Музыка',
              9 : 'Для всей семьи',     
              10 : 'Кулинария',
              11 : 'Прочее'}

# Словарь соответствий тв-программ типам
genre_mapping = {'-': 0,
              'Документальная программа': 3,
              'Познавательная программа': 7,
              'Анимация': 1,
              'Социально-политическая программа': 4,
              'Новости': 4,
              'Развлекательная программа': 5,
              'Кинофильм': 1,
              'Телесериал': 1,
              'Детская программа': 9,
              'Музыкальная программа': 8,
              'Прочее': 11,
              'Сериал (Сериал "Морские дьяволы. Смерч - 2.")': 1,
              'ЗА ГРАНЬЮ': 2, 
              'Место встречи': 4,
              'Сериал (Сериал "Морские дьяволы. Смерч-3")': 1,
              'Ток-шоу «ДНК»': 2,
              'Сериал (Сериал "МОРСКИЕ ДЬЯВОЛЫ. РУБЕЖИ РОДИНЫ")': 1,
              'ОТРИЦАТЕЛИ БОЛЕЗНЕЙ (Научное расследование Сергея Малоземова)': 2,
              'Сериал (Сериал "Морские дьяволы. Смерч")': 1,
              'Новые русские сенсации': 4,
              'Следствие вели...': 3,
              'Обзор. Чрезвычайное происшествие': 4,
              'Сериал (Сериал "Морские дьяволы. Смерч. Судьбы.")': 1,
              'Сегодня/Сегодня в Москве': 4,
              'Сериал (Сериал "Морские дьяволы. Смерч. Судьбы-2")': 1,
              'Сериал (Сериал "АЛЕКС ЛЮТЫЙ. ДЕЛО ШУЛЬЦА")': 1,
              'Сериал (Сериал "Береговая охрана 2")': 1,
              'Сериал (Сериал "Морские дьяволы 3")': 1,
              'Сериал (Сериал "Морские дьяволы 4")': 1,
              'Док. сериал (Д/ф "Знаки судьбы")': 1,
              'Док. сериал (Д/ф "Гадалка")': 1,
              'Док. сериал (Д/ф "СЛЕПАЯ")': 1,
              'Мистические истории': 1,
              'Док. сериал (Д/ф "Уиджи")': 1,
              'Вернувшиеся': 1,
              'Док. сериал (Д/ф "Врачи")': 1,
              'НОВЫЙ ДЕНЬ': 1,
              'Р/б после мультфильмов до M/ф "Возвращение Домовенка"': 9,
              'Своя игра': 5,
              'Сериал (Сериал "Пять минут тишины. Возвращение")': 1,
              'Сериал (Сериал "Морские дьяволы. Судьбы - 2")': 1,
              'Сериал (Сериал "Ментовские войны - 7")': 1,
              'ЖДИ МЕНЯ': 2,
              'Сериал (Сериал "ПЕРВЫЙ ОТДЕЛ")': 1,
              'Шоу «Маска»': 8,
              'Ты не поверишь!': 2,
              'Сериал (Сериал "ПЕРВЫЙ ОТДЕЛ-2")': 1,
              'Документальный фильм (Д/ф "МОИ УНИВЕРСИТЕТЫ. БУДУЩЕЕ ЗА НАСТОЯЩИМ")': 7,
              'Лотерея "У НАС ВЫИГРЫВАЮТ!"': 6,
              'Сериал (Сериал "ПОРТ")': 1,
              'Страна талантов': 2,
              'Сериал (Сериал "Морские дьяволы. Особое задание")': 1,
              'Сериал (Сериал "АНОНИМНЫЙ ДЕТЕКТИВ")': 1,
              'По следу монстра': 1,
              'Сериал (Сериал "Красная зона")': 1,
              'Сериал (Сериал "Шеф 2")': 1,
              'Сериал (Сериал "Морские дьяволы 5")': 1,
              'Сериал (Сериал "Шеф. Новая жизнь")': 1,
              'Сериал (Сериал "Морские дьяволы. Северные рубежи")': 1,
              'Документальный фильм (Д/ф "Анна")': 1,
              'Секрет на миллион': 2,
              'Сериал (Сериал "Балабол-4")': 1,
              'Сериал (Сериал "По ту сторону смерти")': 1,
              'Сериал (Сериал "Сверхъестественное")': 1,
              'Худ. фильм (Х/ф "Хоббит: Пустошь Смауга")': 1,
              'Сериал (Сериал "Ментовские войны - 10")': 1,
              'Сериал (Сериал "Ментовские войны - 11")': 1,
              'Сегодня (вых)': 4,
              'ШОУМАСКГООН': 2,
              'Документальный фильм (Д/ф "ЧЕРНОМОРСКИЙ ЦУГЦВАНГ. ГИБЕЛЬ ТЕПЛОХОДА «АРМЕНИЯ")': 3,
              'Сериал (Сериал "МОРСКИЕ ДЬЯВОЛЫ. ДАЛЬНИЕ РУБЕЖИ")': 1,
              'Док. сериал (Д/ф "Старец")': 1,
              'Добрый день с Валерией': 2,
              'Р/б после мультфильмов до M/ф "В тридесятом веке"': 9,
              'Худ.фильм (Х/ф "Хоббит: Нежданное путешествие")': 1,
              'Худ.фильм (Х/ф "Хоббит: Пустошь Смауга")': 1,
              'Худ. фильм (Х/ф "Хоббит. Битва пяти воинств")': 1,
              'Худ.фильм (Х/ф "Последний легион")': 1,
              'Худ.фильм (Х/ф "Белоснежка: месть гномов")': 1,
              'Худ. фильм (Х/ф "Царство небесное")': 1,
              'Р/б после мультфильмов до M/ф "Влюбчивая ворона"': 9,
              'Звёзды сошлись': 2,
              'Готовим с Алексеем Зиминым': 10,
              'Сериал (Сериал "Невский. Проверка на прочность")': 1,
              'Сериал (Сериал "Проверка на прочность")': 1,
              'Сериал (Сериал "Невский. Чужой среди чужих")': 1,
              'Сериал (Сериал "ГОРЯЧАЯ ТОЧКА-2")': 1,
              'НОВОГОДНЯЯ МАСКА 2022': 2,
              'Сериал (Сериал "Невский")': 1,
              'ЧП.Расследование': 4,
              'Художественный фильм (Х/ф "Чёрный пёс")': 1,
              'Художественный фильм (Х/ф "БАТАЛЬОН")': 1,
              'Сегодня': 4,
              'Сериал (Сериал "Чингачгук")': 1,
              'ОСНОВАНО НА РЕАЛЬНЫХ СОБЫТИЯХ': 4,
              'Сериал (Сериал "Дельфин")': 1,
              'НЕВЕДОМЫЕ ЧУДОВИЩА НА ЗЕМЛЕ (Научное расследование Сергея Малоземова)': 2,
              'Сериал (Сериал "ПОЛИЦЕЙСКОЕ БРАТСТВО")': 1,
              'Сериал (Сериал "ПРАКТИКАНТ 2")': 1,
              'Худ. фильм (Х/ф "Заложница 3")': 1,
              'Худ. фильм (Х/ф "ШУТКИ В СТОРОНУ-2. МИССИЯ В МАЙАМИ")': 1,
              'Художественный фильм (Х/ф "Отставник. Один за всех")': 1,
              'Художественный фильм (Х/ф "Отставник. Спасти врага")': 1,
              'Художественный фильм (Х/ф "ДИНА И ДОБЕРМАН")': 1,
              'Пресс-конференция по итогам встречи министров иностранных дел РФ и Украины': 4,
              '"ТАЙНЫЕ РЕЦЕПТЫ НЕОФИЦИАЛЬНОЙ МЕДИЦИНЫ". Научное расследование Сергея Малозёмова': 2,
              'ТЫ СУПЕР! 60+': 2,
              'Сериал (Сериал "Вспышка")': 1,
              'Сериал (Сериал "Заповедный спецназ")': 1,
              '«ЧТО МОГУТ ЭКСТРАСЕНСЫ?». НАУЧНОЕ РАССЛЕДОВАНИЕ СЕРГЕЯ МАЛОЗЁМОВА': 2}

# Словарь с данными для преобразования колонок
genres_dict = {'genre_map' : genre_map,
               'genre_mapping' : genre_mapping}

# Сохраняем полученный словарь в файл dict_genres.pkl
save_dict(path_main + 'dicts/dict_genres.pkl', genres_dict)

def expand_data(df_data):
    """
    Расширяем входные данные
    Добавляем колонки: spotMonth, weekDay, weekEnd, 
    genre_class (на остове словарей genre_map и genre_mapping)
    :return: Расширенный датафрейм
    """

    # Получаем значения месяцев 1-12
    df_data['spotMonth'] = df_data['spotDay'].dt.month
    # Получаем значения дней недели 0-6
    df_data['weekDay'] = df_data['spotDay'].dt.weekday

    # Получаем значения рабочий/выходной 0
    arr = []
    for day in df_data['spotDay'].dt.weekday:
      if day > 4:
        arr += [1]  # рабочий - 1
      elif np.isnan(day):
        arr += [None]  # дата не определена - NaN
      else: 
        arr += [0]  # выходной - 0
    df_data['weekEnd'] = arr

    # Добавляем колонку genre_class
    arr = []
    for genre in df_data['genre']:
      genre_map = genres_dict['genre_map']
      genre_mapping = genres_dict['genre_mapping'] 
      arr += [genre_map[genre_mapping[genre]]]
    df_data.loc[:,'genre_class'] = arr

    return df_data

df_data = expand_data(df_data)

# Имена колонок с расширенными входными данными
columnsX_plus = columnsX + ['spotMonth', 'weekDay', 'weekEnd']

print(df_data.dtypes)
df_data

"""## Анализирум выходные данные

### Квантили
"""

# Распределение значений в колонке
df_data.quantile([0.8, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 0.995])

"""### Гистограмма"""

# Задаем пространство для подграфиков
f, axes = plt.subplots(5, 1, figsize=(25,15))

# Гистограмма распределения значений в массиве
for i, col_name in enumerate(columnsY):
  color = 'g' 
  if (i == 0): color = 'tab:orange'
  axes[i].hist(df_data[col_name].values, bins=100, color=color) # параметр bins отвечает за количество подгрупп, в которые объединяются данные 
  axes[i].set_ylabel(col_name)

plt.show()

"""### Выбор максимальных значений

Т.к. большая часть выходных значений в БД ближе к нулю, то выберем максимально допустимые значения для колонок 

columnsY = ['AllCalls', 'calls(0)', 'calls(-2)', 'calls(-4)', 'calls(-7)']
"""

# Выберем максимальные значения для выходных данных
columnsY_maxval = {'AllCalls': 200, 
                    'calls(0)': 100, 
                    'calls(-2)': 30, 
                    'calls(-4)': 40, 
                    'calls(-7)': 20}

"""## Обрезаем выходные значения

Обрежем значения в колонках

columnsY = ['AllCalls', 'calls(0)', 'calls(-2)', 'calls(-4)', 'calls(-7)']

которые выше значений в указанных в словаре columnsY_maxval для колонок.


"""

def set_max_dataY(df_data, columnsY, columnsY_maxval):
  """
  Обрезаем масимальные значения выходных данных в колонках columnsY у df_data
  по значениям в columnsY_maxval
  """
  
  # Цикл по выходным колонкам
  for col_name in columnsY:
    df_col = df_data[col_name].copy()                                           # Копируем колонку
    max_val = columnsY_maxval[col_name]                                         # Определяем максимальное значение
    mask = df_col[:] > max_val                                                  # Находим строки, где значения > заданных в columnsY_maxval
    df_col.loc[mask] = max_val                                                  # Присваиваем максимальные значения из columnsY_maxval
    df_data.loc[:, col_name] = df_col                                           # Сохраняем значения в датафрейме


# Обрезаем выходные значения
set_max_dataY(df_data, columnsY, columnsY_maxval)

# Проверяем результат
df_data.describe()

"""## Парсим базу

### dataX, dataY, dataX_dict
"""

def words_to_OHE(df_col):
  '''
    Входной массив преобразуется в OHE
    Размер OHE определяется автоматически

    df_col = ['-', 'РД1', 'РД2', 'РД1', 'РД3', 'РД3', 'РД4', 'РД4']
    words_dict = {'-': 0, 'РД1': 1, 'РД2': 2, 'РД3': 3, 'РД4': 4}
    words_OHE =   array([[0., 0., 0., 0.],    '-' - отбрасывается при приведении к OHE
                         [1., 0., 0., 0.],
                         [0., 1., 0., 0.],
                         [1., 0., 0., 0.],
                         [0., 0., 1., 0.],
                         [0., 0., 1., 0.],
                         [0., 0., 0., 1.],
                         [0., 0., 0., 1.]], dtype=float32)
  
    Все строковые значения, которые не указаны в таблице, значаться как '-'
  '''

  # Удаляем '-' из массива уникальных значений
  unique = df_col.unique()
  if '-' in unique:
    unique = list(unique)
    unique.remove('-')
    unique = np.array(unique)

  # Получаем словарь с уникальными значениями
  words_dict = {j: i+1 for i, j in enumerate(unique)}
  # Принудительно назначаем {'-': 0}
  words_dict['-'] = 0

  # Получаем OHE, где перый элемент отвечает за '-'
  words_OHE = np.array(utils.to_categorical([words_dict[i] for i in df_col.values], num_classes=len(words_dict)))

  # Удаляем первый элемент из OHE. Тогда везде, где встречается '-' будет OHE с нулями [0, 0 ... 0, 0]
  words_OHE = words_OHE[:, 1:]

  #print(words_dict)
  #print(words_OHE)

  return words_OHE, words_dict


def getDataX(df_data):
  '''
    Подготовка датасета по входным параметрам
  '''
  dataX_dict = {}
  advertiser_OHE, dataX_dict['advertiser_dict'] = words_to_OHE(df_data.advertiser)
  product_OHE, dataX_dict['product_dict'] = words_to_OHE(df_data['product'])
  TVChannel_OHE, dataX_dict['TVChannel_dict'] = words_to_OHE(df_data.TVChannel)
  spread_OHE, dataX_dict['spread_dict'] = words_to_OHE(df_data.spread)
  _ , dataX_dict['genre_dict'] = words_to_OHE(df_data.genre)
  genre_class_OHE, dataX_dict['genre_class_dict'] = words_to_OHE(df_data.genre_class)
  spotLine_OHE, dataX_dict['spotLine_dict'] = words_to_OHE(df_data.spotLine)
  spotDuration_OHE, dataX_dict['spotDuration_dict'] = words_to_OHE(df_data.spotDuration)

  # все пустые (не заполненные) ячейки в колонке spotMonth меняем на 13
  df_data.fillna(
      value = {'spotMonth': 13}, 
      inplace=True)
  spotMonth_OHE = utils.to_categorical(df_data['spotMonth'].values - 1, num_classes=13)
  # Удаляем последний элемент из OHE. Тогда везде, где встречается 'NaN' будет OHE с нулями [0, 0 ... 0, 0]
  spotMonth_OHE = spotMonth_OHE[:, :-1]

  # все пустые (не заполненные) ячейки в колонке weekDay меняем на 7
  df_data.fillna(
      value = {'weekDay': 7}, 
      inplace=True)
  weekDay_OHE = utils.to_categorical(df_data['weekDay'].values, num_classes=8)
  # Удаляем последний элемент из OHE. Тогда везде, где встречается 'NaN' будет OHE с нулями [0, 0 ... 0, 0]
  weekDay_OHE = weekDay_OHE[:, :-1]

  # все пустые (не заполненные) ячейки в колонке weekEnd меняем на 2
  df_data.fillna(
      value = {'weekEnd': 2}, 
      inplace=True)
  weekEnd_OHE = utils.to_categorical(df_data['weekEnd'].values, num_classes=3)
  weekEnd_OHE = weekEnd_OHE[:, :-1]

  # Объединяем все OHE в один вектор
  dataX_OHE = np.hstack([advertiser_OHE, product_OHE, TVChannel_OHE, spread_OHE, genre_class_OHE, spotLine_OHE, spotDuration_OHE, spotMonth_OHE, weekDay_OHE, weekEnd_OHE])

  print('dataX_OHE:')
  print(dataX_OHE)
  for x in dataX_dict:
    print(x + ': ', end='')
    print(dataX_dict[x])

  return dataX_OHE, dataX_dict


def getDataY(df_data):
  '''
    Подготовка датасета по выходным параметрам
  '''
  dataY = {}

  for name_col in columnsY:
    dataY[name_col] = df_data[name_col].values.reshape(-1,1)

  print('dataY:')
  print(dataY)
  return dataY

# Получаем dataX в OHE
dataX, dataX_dict = getDataX(df_data)
# Получаем dataY в обычном виде
dataY = getDataY(df_data)

# Сохраняем полученный словарь в файл dictX.pkl
save_dict(path_main + 'dicts/dictX.pkl', dataX_dict)

"""### dataX_for_site_dict"""

"""
  Формируем данные в виде списка для заполнения входных полей на сайте
  [{'name': 'TVChannel':         - название поля
    'type': 'string',            - тип поля (object - string, datetime64 - date, int64 - number)
    'num_classes': 10,           - кол-во классов (длина словаря values)
    'values': {'-': 0,           - словарь со значениями полей и номером класса для OHE
              'ГНЦ': 1,
              'НЦЗ': 2}}, ...]
"""
dataX_for_site = []
for name in columnsX:
  field = {'name': name}                    # название поля

  type_field = '-'
  values = {}
  if df_data[name].dtype == 'O':
    type_field = 'string'
    values = dataX_dict[name + '_dict']
  elif df_data[name].dtype == 'int64':
    type_field = 'number'
  elif df_data[name].dtype == '<M8[ns]':
    type_field = 'date'
  field['type'] = type_field                # тип поля (object - string, datetime64 - date, int64 - number)

  num_classes = len(values)                 

  field['num_classes'] = num_classes        # кол-во классов (длина словаря values)

  values = dict(sorted(values.items()))     # сортировка
  field['values'] = values                  # словарь со значениями полей и номером класса для OHE

  dataX_for_site += [field]                 # добавляем в список

# Сохраняем полученный словарь в файл dataX_for_site.pkl
save_dict(path_main + 'dicts/dataX_for_site.pkl', dataX_for_site)

# пример
dataX_for_site[5:8]

"""## Обучающая и тестовая выборки"""

print('Размерность входных данных: ' + str(dataX.shape))
for n in dataY:
  print('Размерность выходных данных ' + n +': ' + str(dataY[n].shape))

# Пример входных и выходных данных
print(dataX[0])
print(dataY['AllCalls'][0])

"""### Маска train и test"""

# Делим датасет на обучающую и проверочную выборки
# Длина датасета
len_db = len(dataX)                                                         
print('Длина датасета: ', len_db)

# Маски для тренировочной и проверочной выборок
mask_train, mask_test = train_test_split(np.arange(len_db), test_size = 0.1)

print('Длина тренировочной выборки: ', len(mask_train))
print('Длина проверочной выборки: ', len(mask_test))

# Сохраняем полученные маски в файл masks.pkl
save_masks(path_main + 'masks/masks.pkl', mask_train, mask_test)

# Загружаем маски из файла masks.pkl
mask_train, mask_test = load_masks(path_main + 'masks/masks.pkl')

"""### x_train, x_test, y_train, y_test"""

def get_train_test_split(dataX, dataY, mask_train, mask_test, columnsY):
  '''
    Делит датасет на обучающую и проверочную выборки так,
    чтобы для всех выходных данных в dataY значения были из одной строки
  '''
  y_train, y_test = {}, {}

  # Разбиваем на тренировочную и проверочную для dataX
  x_train = dataX[mask_train]
  x_test = dataX[mask_test]

  # Разбиваем на тренировочную и проверочную для dataY
  for col in columnsY:
    y_train[col] = dataY[col][mask_train]
    y_test[col] = dataY[col][mask_test]
 
  return x_train, x_test, y_train, y_test

# Делим датасет на обучающую и проверочную выборки
x_train, x_test, y_train, y_test = get_train_test_split(dataX, dataY, mask_train, mask_test, columnsY)

print('Входные данные')
print(x_train.shape)
print(x_test.shape)
print()
for col_name in columnsY:
  print('Колонка ', col_name)
  print(y_train[col_name].shape)
  print(y_test[col_name].shape)
  print()

"""## Нормализация"""

# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.inverse_transform

def getScalersY(dataY):
  '''
    Создание словаря скеллеров для выходных данных
  '''
  scalersY = {} # список скэйлеров

  for name_col in columnsY:
    scalersY[name_col] = MinMaxScaler()
    scalersY[name_col].fit(dataY[name_col])

    # Сохраняем MinMaxScaler
    path = path_main + 'scalers/' + name_col + '.pkl'
    save_data(path, scalersY[name_col])
    print('Скелер сохранен: ' + path)

  # Сохранить список имён скалеров
  path = path_main + 'scalers/columnsY.pkl'
  save_dict(path, columnsY)
  print('Список скелеров сохранен: ' + path)

  return scalersY


def get_normY(y):
  '''
    Нормализация выходных данных
  '''
  y_norm = {}

  # Загрузить список имён скалеров
  path = path_main + 'scalers/columnsY.pkl'
  columnsY = load_dict(path)
  print('Список скелеров загружен: ' + path)

  for name_col in columnsY:
    # Загружаем скалеры из файлов
    path = path_main + 'scalers/' + name_col + '.pkl'
    scalersY[name_col] = load_data(path)
    print('Скелер загружен: ' + path)

    # Нормализуем данные с помощью скелеров 
    y_norm[name_col] = scalersY[name_col].transform(y[name_col])

  return y_norm

# Создаём скеллеры для выходных данных
scalersY = getScalersY(dataY)

# Норамализуем y_train, y_test
y_train_norm = get_normY(y_train)
y_test_norm = get_normY(y_test)

print('x:')
print(x_train.shape)
print(x_test.shape)
print('y:')
print(y_train['AllCalls'].shape)
print(y_test['AllCalls'].shape)
print('y_norm:')
print(y_train_norm['AllCalls'].shape)
print(y_test_norm['AllCalls'].shape)

# пример норминованных данных
print('Нормированные выходные данные')
print(y_train_norm[columnsY[1]])
# пример преобразования нормализованных выходных данных к нормальному виду
print('НЕ нормированные выходные данные')
print(scalersY[columnsY[1]].inverse_transform(y_train_norm[columnsY[1]]))

"""# Нейронка

## Архитектуры нейронок
"""

ns = {} # Словарь с нейронками

"""### AllCalls """

def createNN_AllCalls():
  '''
    Создаёт структуру НС
  '''
  model = Sequential()
  model.add(BatchNormalization(input_shape=(x_train.shape[1],)))
  model.add(Dense(4096, activation='relu'))
  model.add(Dense(1024, activation='relu'))
  model.add(Dense(512, activation='relu'))
  model.add(Dense(128, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))

  return model


# Обучаем модель полученными данными
def fitNN_AllCalls(model, x_train, y_train, x_test, y_test, 
                   epochs=1000, learning_rate=1e-6):
  '''
    Обучение НС
  '''
  model.compile(optimizer=Adam(learning_rate=learning_rate), 
                loss='mse', metrics=['mae'])

  history = model.fit(x_train, 
                      y_train, 
                      epochs=epochs, 
                      batch_size=100,
                      validation_data=(x_test, y_test))

  plt.plot(history.history['mae'], 
          label='Средняя абсолютная ошибка на обучающем наборе')
  plt.plot(history.history['val_mae'], 
          label='Средняя абсолютная ошибка на проверочном наборе')
  plt.xlabel('Эпоха обучения')
  plt.ylabel('Средняя абсолютная ошибка')
  plt.legend()
  plt.show()

"""### AllCalls 2"""

def createNN_AllCalls():
  '''
    Создаёт структуру НС
  '''
  model = Sequential()
  model.add(BatchNormalization(input_shape=(x_train.shape[1],)))
  model.add(Dropout(0.2))
  model.add(Dense(128, activation='relu'))
  model.add(Dense(16384, activation='tanh'))
  model.add(Dense(64, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(1, activation='sigmoid'))

  return model


# Обучаем модель полученными данными
def fitNN_AllCalls(model, x_train, y_train, x_test, y_test, 
                   epochs=400, learning_rate=1e-4):
  '''
    Обучение НС
  '''
  model.compile(optimizer=Adam(learning_rate=learning_rate), 
                loss='mse', metrics=['mae'])

  history = model.fit(x_train, 
                      y_train, 
                      epochs=epochs, 
                      batch_size=100,
                      validation_data=(x_test, y_test))

  plt.plot(history.history['mae'], 
          label='Средняя абсолютная ошибка на обучающем наборе')
  plt.plot(history.history['val_mae'], 
          label='Средняя абсолютная ошибка на проверочном наборе')
  plt.xlabel('Эпоха обучения')
  plt.ylabel('Средняя абсолютная ошибка')
  plt.legend()
  plt.show()

"""### Calls_n"""

def createNN_Calls_n(epochs):
  '''
    Создаёт структуру НС
  '''
  model = Sequential()
  model.add(BatchNormalization(input_shape=(x_train.shape[1],)))
  model.add(Dense(4096, activation='relu'))
  model.add(Dense(1024, activation='relu'))
  model.add(Dense(512, activation='relu'))
  model.add(Dense(128, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))

  return model


# Обучаем модель полученными данными
def fitNN_Calls_n(model, x_train, y_train, x_test, y_test, 
                   epochs=1000, learning_rate=1e-6):
  '''
    Обучение НС
  '''
  model.compile(optimizer=Adam(learning_rate=learning_rate), 
                loss='mse', metrics=['mae'])

  history = model.fit(x_train, 
                      y_train, 
                      epochs=epochs, 
                      batch_size=100,
                      validation_data=(x_test, y_test))

  plt.plot(history.history['mae'], 
          label='Средняя абсолютная ошибка на обучающем наборе')
  plt.plot(history.history['val_mae'], 
          label='Средняя абсолютная ошибка на проверочном наборе')
  plt.xlabel('Эпоха обучения')
  plt.ylabel('Средняя абсолютная ошибка')
  plt.legend()
  plt.show()

"""### Calls_n 2"""

def createNN_Calls_n():
  '''
    Создаёт структуру НС
  '''
  model = Sequential()
  model.add(BatchNormalization(input_shape=(x_train.shape[1],)))
  model.add(Dropout(0.2))
  model.add(Dense(128, activation='relu'))
  model.add(Dense(16384, activation='tanh'))
  model.add(Dense(64, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(1, activation='sigmoid'))

  return model


# Обучаем модель полученными данными
def fitNN_Calls_n(model, x_train, y_train, x_test, y_test, 
                   epochs=400, learning_rate=1e-4):
  '''
    Обучение НС
  '''
  model.compile(optimizer=Adam(learning_rate=learning_rate), 
                loss='mse', metrics=['mae'])

  history = model.fit(x_train, 
                      y_train, 
                      epochs=epochs, 
                      batch_size=100,
                      validation_data=(x_test, y_test))

  plt.plot(history.history['mae'], 
          label='Средняя абсолютная ошибка на обучающем наборе')
  plt.plot(history.history['val_mae'], 
          label='Средняя абсолютная ошибка на проверочном наборе')
  plt.xlabel('Эпоха обучения')
  plt.ylabel('Средняя абсолютная ошибка')
  plt.legend()
  plt.show()

"""## Обучение.

### Создать заново все модели
"""

# Создать заново всё модели
ns['AllCalls'] = createNN_AllCalls()
ns['calls(0)'] = createNN_Calls_n()
ns['calls(-2)'] = createNN_Calls_n()
ns['calls(-4)'] = createNN_Calls_n()
ns['calls(-7)'] = createNN_Calls_n()

"""### AllCalls"""

ns_name = 'AllCalls'                                                                              # Название НС
ns[ns_name] = createNN_AllCalls()                                                                 # Создать заново НС
fitNN_AllCalls(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name])         # Обучить НС

# Дообучение
ns_name = 'AllCalls'                                                                              # Название НС
fitNN_AllCalls(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name],         # Дообучить НС
               epochs=50, learning_rate=1e-5)

"""### Calls(0)"""

ns_name = 'calls(0)'                                                                              # Название НС
ns[ns_name] = createNN_Calls_n()                                                                  # Создать заново НС
fitNN_Calls_n(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name])          # Обучить НС

# Дообучение
ns_name = 'calls(0)'                                                                              # Название НС
fitNN_Calls_n(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name],          # Дообучить НС
               epochs=50, learning_rate=1e-5)

"""### Calls(-2)"""

ns_name = 'calls(-2)'                                                                             # Название НС
ns[ns_name] = createNN_Calls_n()                                                                  # Создать заново НС
fitNN_Calls_n(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name],          # Обучить НС
                             epochs=150)

# Дообучение
ns_name = 'calls(-2)'                                                                             # Название НС
fitNN_Calls_n(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name],          # Дообучить НС
               epochs=50, learning_rate=1e-5)

"""### Calls(-4)"""

ns_name = 'calls(-4)'                                                                             # Название НС
ns[ns_name] = createNN_Calls_n()                                                                  # Создать заново НС
fitNN_Calls_n(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name],          # Обучить НС
                             epochs=150)

# Дообучение
ns_name = 'calls(-4)'                                                                             # Название НС
fitNN_Calls_n(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name],          # Дообучить НС
               epochs=50, learning_rate=1e-5)

"""### Calls(-7)"""

ns_name = 'calls(-7)'                                                                             # Название НС
ns[ns_name] = createNN_Calls_n()                                                                  # Создать заново НС
fitNN_Calls_n(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name],          # Обучить НС
                             epochs=250)

# Дообучение
ns_name = 'calls(-7)'                                                                              # Название НС
fitNN_Calls_n(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name],          # Дообучить НС
               epochs=50, learning_rate=1e-5)

"""## Сохраняем НС вместе с весами"""

# Список имён моделей
names_model = list(ns.keys())

# Сохраняем список имён моделей
save_dict(path_main + 'models/name_models.pkl', names_model)

# Сохраняем модели
for name_model in names_model:
    ns[name_model].save(path_main + 'models/' + name_model + '.h5')

"""## Загрузить НС вместе с весами"""

# Получить список имён моделей
names_model = load_dict(path_main + 'models/name_models.pkl')

# Загружаем модели
ns = {}
for name_model in names_model:
    ns[name_model] = load_model(path_main + 'models/' + name_model + '.h5')

"""## Проверка результата MAE

### Функции проверки
"""

# словарь абсолютных средних ошибок
mae = {}

def calc_MAE_error_model(ns_name, select='test'):
  '''
    Подсчёт MAE и ошибки 
    select - 'test', 'train'
  '''
  model = ns[ns_name]
  scaler = scalersY[ns_name]

  if select == 'test':
    x = x_test
    y = y_test[ns_name]
  elif select == 'train':
    x = x_train
    y = y_train[ns_name]

  predict_norm = model.predict(x)                                       # результат в нормализованном виде
  predict = scaler.inverse_transform(predict_norm).astype('float64')    # преобразование нормализованных выходных данных к нормальному виду
  

  err = abs(y - predict)                                                # считаем абсолютную ошибку

  mae = round(err.mean(), 2)                                            # считаем MAE - среднюю абсолютную ошибку

  return predict, err, mae 


def get_tables_result(ns_names):
  '''
    Возвращает таблицы с результатами подсчёта MAE для выбранных НС
  '''
  # таблица для MAE
  df_mae = pd.DataFrame(data=ns_names, columns=['ns_name'])
  df_mae['MAE_test'] = ''
  df_mae['MAE_train'] = ''

  # длина таблицы по максимальным данным
  num_row = max([len(y_test[columnsY[0]]), len(y_train[columnsY[0]])])
  
  # таблица для err с кол-вом строк num_row
  df_err = pd.DataFrame(index=[i for i in range(num_row)])

  for ns_name in ns_names:
    for select in ['test', 'train']:
      predict, err, mae = calc_MAE_error_model(ns_name, select)
      name_col = ns_name + '_' + select + '_'

      # дополнить нулями если нужно
      num_add_row = num_row - len(predict)
      predict = np.append(predict, np.zeros((num_add_row)))
      err = np.append(err, np.zeros((num_add_row)))

      predict = np.round(predict, 1)
      err = np.round(err, 1)
     
      # Формируем таблицу
      if select == 'test':
        df_err[name_col + 'true'] = np.round(np.append(y_test[ns_name], np.zeros((num_add_row))), 0)
        df_mae.loc[df_mae.ns_name == ns_name, 'MAE_test'] = mae
      elif select == 'train':
        df_err[name_col + 'true'] = np.round(np.append(y_train[ns_name], np.zeros((num_add_row))), 0)
        df_mae.loc[df_mae.ns_name == ns_name, 'MAE_train'] = mae

      df_err[name_col + 'pred'] = predict
      df_err[name_col + 'err'] = err

  return df_mae, df_err

"""### Ошибка и MAE"""

# Получаем результат
df_mae, df_err = get_tables_result(columnsY)

# Таблица ошибок по разным НС
'''
  AllCalls      - имя НС
  test, train   - проверочная или тренировочная выборки
  true          - табличное (правильное)  значение
  pred          - передсказанное значение
  err           - абсолютная ошибка
'''
df_err.head(25)

# Таблица MAE
'''
  # MAE_test - на проверочной выборке
  # MAE_train - на обучающей выборке
'''
df_mae

"""### Вывести результат по одной НС"""

# Выберите имя НС через индекс массива (0-4)
num = 3
ns_name = ['AllCalls', 'calls(0)', 'calls(-2)', 'calls(-4)', 'calls(-7)']

df_err[df_err.columns[df_err.columns.str.find(ns_name[num]) == 0]].head(30)

"""# ВЫВОД

Корреляция есть. MAE неплохой для старта.

# Подготовка данных для предикта
"""

def words_to_OHE_with_dict(df_col, words_dict):
    """
    Входной массив преобразуется в OHE
    Размер OHE определяется автоматически

    df_col = ['-', 'РД1', 'РД2', 'РД1', 'РД3', 'РД3', 'РД4', 'РД4']
    words_dict = {'-': 0, 'РД1': 1, 'РД2': 2, 'РД3': 3, 'РД4': 4}
    words_OHE =   array([[0., 0., 0., 0.],    '-' - отбрасывается при приведении к OHE
                         [1., 0., 0., 0.],
                         [0., 1., 0., 0.],
                         [1., 0., 0., 0.],
                         [0., 0., 1., 0.],
                         [0., 0., 1., 0.],
                         [0., 0., 0., 1.],
                         [0., 0., 0., 1.]], dtype=float32)

    Все строковые значения, которые не указаны в таблице, значаться как '-'
    """

    # Получаем OHE, где перый элемент отвечает за '-'
    words_OHE = np.array(utils.to_categorical([words_dict[i] for i in df_col.values], num_classes=len(words_dict)))

    # Удаляем первый элемент из OHE. Тогда везде, где встречается '-' будет OHE с нулями [0, 0 ... 0, 0]
    words_OHE = words_OHE[:, 1:]

    return words_OHE


def prepareDataX(df_data, dataX_dict):
    """
      Подготовка датасета по входным параметрам
      df_data - входной датасет
      dataX_dict - словарь для преобазования в OHE

      return: массив в OHE
    """

    advertiser_OHE = words_to_OHE_with_dict(df_data.advertiser, dataX_dict['advertiser_dict'])
    product_OHE = words_to_OHE_with_dict(df_data['product'], dataX_dict['product_dict'])
    TVChannel_OHE = words_to_OHE_with_dict(df_data.TVChannel, dataX_dict['TVChannel_dict'])
    spread_OHE = words_to_OHE_with_dict(df_data.spread, dataX_dict['spread_dict'])
    genre_class_OHE = words_to_OHE_with_dict(df_data.genre_class, dataX_dict['genre_class_dict'])
    spotLine_OHE = words_to_OHE_with_dict(df_data.spotLine, dataX_dict['spotLine_dict'])
    spotDuration_OHE = words_to_OHE_with_dict(df_data.spotDuration, dataX_dict['spotDuration_dict'])

    # все пустые (не заполненные) ячейки в колонке spotMonth меняем на 13
    df_data.fillna(
        value={'spotMonth': 13},
        inplace=True)
    spotMonth_OHE = utils.to_categorical(df_data['spotMonth'].values - 1, num_classes=13)
    # Удаляем последний элемент из OHE. Тогда везде, где встречается 'NaN' будет OHE с нулями [0, 0 ... 0, 0]
    spotMonth_OHE = spotMonth_OHE[:, :-1]

    # все пустые (не заполненные) ячейки в колонке weekDay меняем на 7
    df_data.fillna(
        value={'weekDay': 7},
        inplace=True)
    weekDay_OHE = utils.to_categorical(df_data['weekDay'].values, num_classes=8)
    # Удаляем последний элемент из OHE. Тогда везде, где встречается 'NaN' будет OHE с нулями [0, 0 ... 0, 0]
    weekDay_OHE = weekDay_OHE[:, :-1]

    # все пустые (не заполненные) ячейки в колонке weekEnd меняем на 2
    df_data.fillna(
        value={'weekEnd': 2},
        inplace=True)
    weekEnd_OHE = utils.to_categorical(df_data['weekEnd'].values, num_classes=3)
    weekEnd_OHE = weekEnd_OHE[:, :-1]

    # Объединяем все OHE в один вектор
    dataX_OHE = np.hstack(
        [advertiser_OHE, product_OHE, TVChannel_OHE, spread_OHE, genre_class_OHE, spotLine_OHE, spotDuration_OHE, weekDay_OHE,
         spotMonth_OHE, weekDay_OHE, weekEnd_OHE])

    return dataX_OHE

# Подготавливаем dataX в OHE для предикта
input_dataX = prepareDataX(df_data, dataX_dict)