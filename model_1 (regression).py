# -*- coding: utf-8 -*-
"""Регрессия. Анализ результатов ТВ-рекламы. Стажировка Mediabuyer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vTSyyCBDGro7viY48vSbGREyHMS_F6an

# Корневой каталог
"""

# Задание пути к основной рабочей папке

path_main = '/content/drive/MyDrive/стажировка/AntonR_Regression/'

"""# Импорт Библиотек"""

import numpy as np #Библиотека работы с массивами
import pandas as pd # Библиотека для работы с базами
import gdown

from tensorflow.keras.models import Sequential, Model, load_model 
from tensorflow.keras.layers import concatenate, Input, Dense, Dropout, BatchNormalization, Flatten #
from tensorflow.keras import utils #Используем для to_categorical
from tensorflow.keras.optimizers import Adam,Adadelta,SGD,Adagrad,RMSprop #
from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence #
from tensorflow.keras.preprocessing.sequence import pad_sequences #
from tensorflow.keras.callbacks import LambdaCallback # подключаем колбэки

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split # Для разбивки на выборки

import matplotlib.pyplot as plt #

import pickle # для записи словаря в файл в формате JSON
import joblib # для сохранения MinMaxScaler

"""# Функции

## Сохранить|загрузить словарь
"""

def save_dict(path_name, data_dict):
  """
  Сохраняет словарь в файл
  path_name - путь к файлу
  data_dict - словарь

  Описание https://docs-python.ru/standart-library/modul-pickle-python/
  """

  with open(path_name, 'wb') as f:
    pickle.dump(data_dict, f)

  
def load_dict(path_name):
  """
  Загружает словарь из файла
  path_name - путь к файлу

  Возвращает:
  data_dict - словарь

  Описание https://docs-python.ru/standart-library/modul-pickle-python/
  """

  with open(path_name, 'rb') as f:
    return pickle.load(f)

"""# Подготовка БД"""

# Подключаем google-диск
from google.colab import drive
drive.mount('/content/drive')

"""##Загружаем базу и смотрим содержание"""

# Загрузка датасета из облака (линейка 10 минут)
df_db = pd.read_csv(path_main + 'Копия линейки со звонками - РС со звонками за 10мин.csv', sep=',')
df_db.head(5)

df_db.info()

"""##Оставляем только нужное из базы"""

# Отбрасываем данные с пустыми ячейками
df_db.drop(['IDMediscope', 'IDTricollor'], axis=1, inplace=True)
df_db.dropna(axis=0, how='any', inplace=True)
df_db = df_db.loc[(df_db['AllCalls'] < 200)]    # Отсекаем 3% датасета

# Имена колонок с входными данными
columnsX = ['advertiser', 'product', 'TVChannel', 'spread', 'genre', 'spotLine', 'spotDay', 'spotDuration']
# Имена колонок с выходными данными
columnsY = ['AllCalls', 'calls(0)', 'calls(-2)', 'calls(-4)', 'calls(-7)']

columns = columnsX + columnsY
df_data = pd.DataFrame(data=df_db[columns], index=None, columns=columns, dtype=None)
df_data['spotDay'] = pd.to_datetime(df_data['spotDay'], errors='coerce') # задаём тип столбцу spotDay

# колонку spotDuration переводим в тип object
df_data['spotDuration'] = df_data['spotDuration'].astype(str)

# Перевод spotLine в формат 00:00:00, чтобы удобно было сортировать
df_data['spotLine'] = pd.to_datetime(df_data['spotLine']) # задаём тип столбцу spotLine
df_data['spotLine'] = df_data['spotLine'].dt.strftime('%H:%M:%S')

print(df_data.dtypes)
df_data

"""## Расширяем базу"""

# Создание колонок типа datetime
df_data['spotMonth'] = df_data['spotDay'].dt.month
df_data['weekDay'] = df_data['spotDay'].dt.weekday

arr = []
for day in df_data['spotDay'].dt.weekday:
  if day > 4:
    arr += [1]  # уикэнд - 1
  # elif np.isnan(day):
  #   arr += [None]  # дата не определена - NaN
  else: 
    arr += [0]  # рабочий - 0
df_data['weekEnd'] = arr

# обобщение видов рекламируемых продуктов
products_mapping = {'КЛАРЕОЛ': 2,
                    'СУСТАЗДРАВ': 2,
                    'YUNA': 5,
                    'АЛОЭ ВЕРА матрас-топпер': 1,
                    'ЧУДО-СЛУХ ПРО': 4,
                    'ЗДОРОВЬЕ ПЛЮС (СМАРТ-БРАСЛЕТ)': 4,
                    'МАССАЖНАЯ ПОДУШКА': 1,
                    'сыворотка': 2,
                    'ЗДОРОВЬЕ ПЛЮС (СМАРТ-БРАСЛЕТ), TOP SHOP': 4,
                    'БЫСТРОЕ ТЕПЛО обогреватель': 3,
                    'термобельё': 5,
                    'швабра': 1,
                    'отпугиватель грызунов': 1,
                    'ШТОРЫ': 1,
                    'ПАРНИК': 1,
                    'спрей-швабра': 1,
                    'культиватор': 1,
                    'Мини-велотренажёр': 6,
                    'СОЮЗ-АПОЛЛОН': 2,
                    'Мозг Терапи': 2,
                    'СЕКРЕТ ИМПЕРАТОРА': 2,
                    'MOVEMENT': 6,
                    'NICER-DICER мультипечь': 3,
                    'пылесос': 3,
                    'измельчитель': 3,
                    'ФЛЕБОЗОЛ': 2,
                    'КОТТЕДЖНЫЙ ПОСЕЛОК': 7,
                    'Movement Gel': 2,
                    'усилитель звука': 3,
                    'нагреватель': 1,
                    'браслет': 4,
                    'велотренажёр': 6,
                    'Травопар': 2,
                    'реппелер': 1}
df_data['products_map'] = df_data['product'].map(products_mapping)

# Обобщение типов тв-программ
genre_mapping = {'Документальная программа': 3,
              'Познавательная программа': 7,
              'Анимация': 1,
              'Социально-политическая программа': 4,
              'Новости': 4,
              'Развлекательная программа': 5,
              'Кинофильм': 1,
              'Телесериал': 1,
              'Детская программа': 9,
              'Музыкальная программа': 8,
              'Прочее': 11,
              'Сериал (Сериал "Морские дьяволы. Смерч - 2.")': 1,
              'ЗА ГРАНЬЮ': 2,
              'Место встречи': 4,
              'Сериал (Сериал "Морские дьяволы. Смерч-3")': 1,
              'Ток-шоу «ДНК»': 2,
              'Сериал (Сериал "МОРСКИЕ ДЬЯВОЛЫ. РУБЕЖИ РОДИНЫ")': 1,
              'ОТРИЦАТЕЛИ БОЛЕЗНЕЙ (Научное расследование Сергея Малоземова)': 2,
              'Сериал (Сериал "Морские дьяволы. Смерч")': 1,
              'Новые русские сенсации': 4,
              'Следствие вели...': 3,
              'Обзор. Чрезвычайное происшествие': 4,
              'Сериал (Сериал "Морские дьяволы. Смерч. Судьбы.")': 1,
              'Сегодня/Сегодня в Москве': 4,
              'Сериал (Сериал "Морские дьяволы. Смерч. Судьбы-2")': 1,
              'Сериал (Сериал "АЛЕКС ЛЮТЫЙ. ДЕЛО ШУЛЬЦА")': 1,
              'Сериал (Сериал "Береговая охрана 2")': 1,
              'Сериал (Сериал "Морские дьяволы 3")': 1,
              'Сериал (Сериал "Морские дьяволы 4")': 1,
              'Док. сериал (Д/ф "Знаки судьбы")': 1,
              'Док. сериал (Д/ф "Гадалка")': 1,
              'Док. сериал (Д/ф "СЛЕПАЯ")': 1,
              'Мистические истории': 1,
              'Док. сериал (Д/ф "Уиджи")': 1,
              'Вернувшиеся': 1,
              'Док. сериал (Д/ф "Врачи")': 1,
              'НОВЫЙ ДЕНЬ': 1,
              'Р/б после мультфильмов до M/ф "Возвращение Домовенка"': 9,
              'Своя игра': 5,
              'Сериал (Сериал "Пять минут тишины. Возвращение")': 1,
              'Сериал (Сериал "Морские дьяволы. Судьбы - 2")': 1,
              'Сериал (Сериал "Ментовские войны - 7")': 1,
              'ЖДИ МЕНЯ': 2,
              'Сериал (Сериал "ПЕРВЫЙ ОТДЕЛ")': 1,
              'Шоу «Маска»': 8,
              'Ты не поверишь!': 2,
              'Сериал (Сериал "ПЕРВЫЙ ОТДЕЛ-2")': 1,
              'Документальный фильм (Д/ф "МОИ УНИВЕРСИТЕТЫ. БУДУЩЕЕ ЗА НАСТОЯЩИМ")': 7,
              'Лотерея "У НАС ВЫИГРЫВАЮТ!"': 6,
              'Сериал (Сериал "ПОРТ")': 1,
              'Страна талантов': 2,
              'Сериал (Сериал "Морские дьяволы. Особое задание")': 1,
              'Сериал (Сериал "АНОНИМНЫЙ ДЕТЕКТИВ")': 1,
              'По следу монстра': 1,
              'Сериал (Сериал "Красная зона")': 1,
              'Сериал (Сериал "Шеф 2")': 1,
              'Сериал (Сериал "Морские дьяволы 5")': 1,
              'Сериал (Сериал "Шеф. Новая жизнь")': 1,
              'Сериал (Сериал "Морские дьяволы. Северные рубежи")': 1,
              'Документальный фильм (Д/ф "Анна")': 1,
              'Секрет на миллион': 2,
              'Сериал (Сериал "Балабол-4")': 1,
              'Сериал (Сериал "По ту сторону смерти")': 1,
              'Сериал (Сериал "Сверхъестественное")': 1,
              'Худ. фильм (Х/ф "Хоббит: Пустошь Смауга")': 1,
              'Сериал (Сериал "Ментовские войны - 10")': 1,
              'Сериал (Сериал "Ментовские войны - 11")': 1,
              'Сегодня (вых)': 4,
              'ШОУМАСКГООН': 2,
              'Документальный фильм (Д/ф "ЧЕРНОМОРСКИЙ ЦУГЦВАНГ. ГИБЕЛЬ ТЕПЛОХОДА «АРМЕНИЯ")': 3,
              'Сериал (Сериал "МОРСКИЕ ДЬЯВОЛЫ. ДАЛЬНИЕ РУБЕЖИ")': 1,
              'Док. сериал (Д/ф "Старец")': 1,
              'Добрый день с Валерией': 2,
              'Р/б после мультфильмов до M/ф "В тридесятом веке"': 9,
              'Худ.фильм (Х/ф "Хоббит: Нежданное путешествие")': 1,
              'Худ.фильм (Х/ф "Хоббит: Пустошь Смауга")': 1,
              'Худ. фильм (Х/ф "Хоббит. Битва пяти воинств")': 1,
              'Худ.фильм (Х/ф "Последний легион")': 1,
              'Худ.фильм (Х/ф "Белоснежка: месть гномов")': 1,
              'Худ. фильм (Х/ф "Царство небесное")': 1,
              'Р/б после мультфильмов до M/ф "Влюбчивая ворона"': 9,
              'Звёзды сошлись': 2,
              'Готовим с Алексеем Зиминым': 10,
              'Сериал (Сериал "Невский. Проверка на прочность")': 1,
              'Сериал (Сериал "Проверка на прочность")': 1,
              'Сериал (Сериал "Невский. Чужой среди чужих")': 1,
              'Сериал (Сериал "ГОРЯЧАЯ ТОЧКА-2")': 1,
              'НОВОГОДНЯЯ МАСКА 2022': 2,
              'Сериал (Сериал "Невский")': 1,
              'ЧП.Расследование': 4,
              'Художественный фильм (Х/ф "Чёрный пёс")': 1,
              'Художественный фильм (Х/ф "БАТАЛЬОН")': 1,
              'Сегодня': 4,
              'Сериал (Сериал "Чингачгук")': 1,
              'ОСНОВАНО НА РЕАЛЬНЫХ СОБЫТИЯХ': 4,
              'Сериал (Сериал "Дельфин")': 1,
              'НЕВЕДОМЫЕ ЧУДОВИЩА НА ЗЕМЛЕ (Научное расследование Сергея Малоземова)': 2,
              'Сериал (Сериал "ПОЛИЦЕЙСКОЕ БРАТСТВО")': 1,
              'Сериал (Сериал "ПРАКТИКАНТ 2")': 1,
              'Худ. фильм (Х/ф "Заложница 3")': 1,
              'Худ. фильм (Х/ф "ШУТКИ В СТОРОНУ-2. МИССИЯ В МАЙАМИ")': 1,
              'Художественный фильм (Х/ф "Отставник. Один за всех")': 1,
              'Художественный фильм (Х/ф "Отставник. Спасти врага")': 1,
              'Художественный фильм (Х/ф "ДИНА И ДОБЕРМАН")': 1,
              'Пресс-конференция по итогам встречи министров иностранных дел РФ и Украины': 4,
              '"ТАЙНЫЕ РЕЦЕПТЫ НЕОФИЦИАЛЬНОЙ МЕДИЦИНЫ". Научное расследование Сергея Малозёмова': 2,
              'ТЫ СУПЕР! 60+': 2,
              'Сериал (Сериал "Вспышка")': 1,
              'Сериал (Сериал "Заповедный спецназ")': 1,
              '«ЧТО МОГУТ ЭКСТРАСЕНСЫ?». НАУЧНОЕ РАССЛЕДОВАНИЕ СЕРГЕЯ МАЛОЗЁМОВА': 2}
df_data['genre_map'] = df_data['genre'].map(genre_mapping)

# Имена колонок с расширенными входными данными, новые: 'products_map' 'genre_map'
columnsX_plus = columnsX + ['spotMonth', 'weekDay', 'weekEnd', 'products_map','genre_map']

print(df_data.dtypes)
df_data

"""## Обрабатываем базу

## Обучающая и тестовая выборки
"""

def words_to_OHE(df_col):
  '''
    Входной массив преобразуется в OHE
    Размер OHE определяется автоматически

    df_col = ['РД1', 'РД2', 'РД1', 'РД3', 'РД3', 'РД4', 'РД4']
    words_dict = {'РД1': 0, 'РД2': 1, 'РД3': 2, 'РД4': 3}
    words_OHE =   array([[1., 0., 0., 0.],
                        [0., 1., 0., 0.],
                        [1., 0., 0., 0.],
                        [0., 0., 1., 0.],
                        [0., 0., 1., 0.],
                        [0., 0., 0., 1.],
                        [0., 0., 0., 1.]], dtype=float32)
  '''
  words_dict = {j: i for i, j in enumerate(df_col.unique())}
  words_OHE = np.array(utils.to_categorical([words_dict[i] for i in df_col.values]))

  #print(words_dict)
  #print(words_OHE)

  return words_OHE, words_dict


def getDataX(df_data):
  '''
    Подготовка датасета по входным параметрам
  '''
  dataX_dict = {}
  advertiser_OHE, dataX_dict['advertiser_dict'] = words_to_OHE(df_data.advertiser)
  product_OHE, dataX_dict['product_dict'] = words_to_OHE(df_data['product'])
  TVChannel_OHE, dataX_dict['TVChannel_dict'] = words_to_OHE(df_data.TVChannel)
  spread_OHE, dataX_dict['spread_dict'] = words_to_OHE(df_data.spread)
  genre_OHE, dataX_dict['genre_dict'] = words_to_OHE(df_data.genre)
  spotLine_OHE, dataX_dict['spotLine_dict'] = words_to_OHE(df_data.spotLine)
  spotDuration_OHE, dataX_dict['spotDuration_dict'] = words_to_OHE(df_data.spotDuration)

  spotMonth_OHE = utils.to_categorical(df_data['spotMonth'].values - 1, num_classes=12)
  weekDay_OHE = utils.to_categorical(df_data['weekDay'].values, num_classes=7)
  weekEnd_OHE = utils.to_categorical(df_data['weekEnd'].values, num_classes=2)
    
  # обощение видов рекламируемых продуктов
  dataX_dict['producttypes_dict'] = {j: i for i, j in enumerate(df_data['products_map'].unique())}
  producttypes_OHE = utils.to_categorical(df_data['products_map'].values - 1, num_classes=7)

  # Обобщение типов тв-программ
  dataX_dict['genretypes_dict'] = {j: i for i, j in enumerate(df_data['genre_map'].unique())}
  genretypes_OHE = utils.to_categorical(df_data['genre_map'].values - 1, num_classes=11)

  # Объединяем все OHE в один вектор
  dataX = np.hstack([advertiser_OHE, TVChannel_OHE, product_OHE, genre_OHE,
                     spread_OHE, spotLine_OHE, spotDuration_OHE, weekDay_OHE,
                     spotMonth_OHE, weekEnd_OHE, producttypes_OHE, genretypes_OHE])

  print('dataX:')
  print(dataX.shape)
  for x in dataX_dict:
    print(x + ': ', end='')
    print(dataX_dict[x])

  return dataX, dataX_dict


def getDataY(df_data):
  '''
    Подготовка датасета по выходным параметрам
  '''
  dataY = {}

  for name_col in columnsY:
    dataY[name_col] = df_data[name_col].values.reshape(-1,1)

  print('dataY:')
  print(dataY)
  return dataY

# Получаем dataX в OHE
dataX, dataX_dict = getDataX(df_data)
# Получаем dataY в обычном виде
dataY = getDataY(df_data)

# Сохраняем полученный словарь в файл dict.pkl
save_dict(path_main + 'dicts/dict.pkl', dataX_dict)

"""### dataX_for_site_dict"""

"""
  Формируем данные в виде списка для заполнения входных полей на сайте
  [{'name': 'TVChannel':         - название поля
    'type': 'string',            - тип поля (object - string, datetime64 - date, int64 - number)
    'num_classes': 10,           - кол-во классов (длина словаря values)
    'values': {'-': 0,           - словарь со значениями полей и номером класса для OHE
              'ГНЦ': 1,
              'НЦЗ': 2}}, ...]
"""
dataX_for_site = []
for name in columnsX:
  field = {'name': name}                    # название поля

  type_field = '-'
  values = {}
  if df_data[name].dtype == 'O':
    type_field = 'string'
    values = dataX_dict[name + '_dict']
  elif df_data[name].dtype == 'int64':
    type_field = 'number'
  elif df_data[name].dtype == '<M8[ns]':
    type_field = 'date'
  field['type'] = type_field                # тип поля (object - string, datetime64 - date, int64 - number)

  num_classes = len(values)                 

  field['num_classes'] = num_classes        # кол-во классов (длина словаря values)

  values = dict(sorted(values.items()))     # сортировка
  field['values'] = values                  # словарь со значениями полей и номером класса для OHE

  dataX_for_site += [field]                 # добавляем в список

# Сохраняем полученный словарь в файл dataX_for_site.pkl
save_dict(path_main + 'dicts/dataX_for_site.pkl', dataX_for_site)

# пример
dataX_for_site[6:8]

print('Размерность входных данных: ' + str(dataX.shape))
for n in dataY:
  print('Размерность выходных данных ' + n +': ' + str(dataY[n].shape))

# Пример входных и выходных данных
print(dataX[0])
print(dataY['AllCalls'][0])

# Функция делит датасет на обучающую и проверочную выборки
def get_train_test_split(dataX, dataY, test_size=0.1):
  '''
    Делит датасет на обучающую и проверочную выборки так,
    чтобы для всех выходных данных в dataY значения были из одной строки
  '''
  # Вытаскиваем список имён в словаре, чтобы сохранить порядок при прохождении в цикле
  # т.к. цикл по словарю может быть в разном порядке
  name_col_dataY = [name for name in dataY]
  len_name_col_dataY = len(name_col_dataY)

  # Объединяем весь словарь в один массив, склеивая столбцы
  dataY_All = dataY[name_col_dataY[0]]
  for name_col in name_col_dataY[1:]:
    dataY_All = np.hstack([dataY_All, dataY[name_col]]) 

  # Разбиваем на тренировочную и проверочную
  x_train, x_test, y_train_All, y_test_All = train_test_split(dataX, dataY_All, test_size=test_size) 

  # Разделяем обратно массив в словарь, возвращая столбцы на своё место
  y_train, y_test = {}, {}
  y_train_All = np.hsplit(y_train_All, len_name_col_dataY)
  y_test_All = np.hsplit(y_test_All, len_name_col_dataY)
  for i in range(len_name_col_dataY):
    y_train[name_col_dataY[i]] = y_train_All[i]
    y_test[name_col_dataY[i]] = y_test_All[i]
  
  return x_train, x_test, y_train, y_test

# Делим датасет на обучающую и проверочную выборки
x_train, x_test, y_train, y_test = get_train_test_split(dataX, dataY, test_size = 0.10)

print(x_train.shape)
print(x_test.shape)
print(y_train['AllCalls'].shape)
print(y_test['AllCalls'].shape)

"""## Нормализация"""

# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.inverse_transform

def getScalersY(dataY):
  '''
    Создание словаря скеллеров для выходных данных
  '''
  scalersY = {} # список скэйлеров

  for name_col in columnsY:
    scalersY[name_col] = MinMaxScaler()
    scalersY[name_col].fit(dataY[name_col])

    # Сохраняем MinMaxScaler
    path = path_main + 'scalers/' + name_col + '.pkl'
    with open(path, 'wb') as f:
      pickle.dump(scalersY[name_col], f)
    print('Скелер сохранен: ' + path)

  # Сохранить список имён скалеров
  path = path_main + 'scalers/columnsY.pkl'
  save_dict(path, columnsY)
  print('Список скелеров сохранен: ' + path)

  return scalersY


def get_normY(y):
  '''
    Нормализация выходных данных
  '''
  y_norm = {}

  # Загрузить список имён скалеров
  path = path_main + 'scalers/columnsY.pkl'
  columnsY = load_dict(path)
  print('Список скелеров загружен: ' + path)

  for name_col in columnsY:
    # Загружаем скалеры из файлов
    path = path_main + 'scalers/' + name_col + '.pkl'
    with open(path, 'rb') as f:
      scalersY[name_col] = pickle.load(f)
    print('Скелер загружен: ' + path)

    y_norm[name_col] = scalersY[name_col].transform(y[name_col])

  return y_norm

# Создаём скеллеры для выходных данных
scalersY = getScalersY(dataY)

# Нормализуем y_train, y_test
y_train_norm = get_normY(y_train)
y_test_norm = get_normY(y_test)

print('x:')
print(x_train.shape)
print(x_test.shape)
print('y:')
print(y_train['AllCalls'].shape)
print(y_test['AllCalls'].shape)
print('y_norm:')
print(y_train_norm['AllCalls'].shape)
print(y_test_norm['AllCalls'].shape)

# пример норминованных данных
print('Нормированные выходные данные')
print(y_train_norm[columnsY[1]])
# пример преобразования нормализованных выходных данных к нормальному виду
print('НЕ нормированные выходные данные')
print(scalersY[columnsY[1]].inverse_transform(y_train_norm[columnsY[1]]))

"""# Нейронка

## Архитектуры нейронок
"""

ns = {} # Словарь с нейронками

"""### AllCalls"""

def createNN_AllCalls():
  '''
    Создаёт структуру НС
  '''

  model = Sequential()
  model.add(BatchNormalization(input_shape=(x_train.shape[1],)))
  model.add(Dropout(0.2))
  model.add(Dense(128, activation='relu'))
  model.add(Dense(16384, activation='tanh'))
  model.add(Dense(64, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(1, activation='sigmoid'))

  model.summary()
  return model


# Обучаем модель полученными данными
def fitNN_AllCalls(model, x_train, y_train, x_test, y_test):
  '''
    Обучение НС
  '''
  model.compile(optimizer=Adam(learning_rate=1e-6), loss='mse', metrics=['mae'])

  history = model.fit(x_train, 
                      y_train, 
                      epochs=1000, 
                      batch_size=100,
                      validation_data=(x_test, y_test))

  plt.plot(history.history['mae'], 
          label='Средняя абсолютная ошибка на обучающем наборе')
  plt.plot(history.history['val_mae'], 
          label='Средняя абсолютная ошибка на проверочном наборе')
  plt.xlabel('Эпоха обучения')
  plt.ylabel('Средняя абсолютная ошибка')
  plt.legend()
  plt.show()
  model.save('model_AllCalls.h5')

"""### Calls_n"""

def createNN_Calls_n():
  '''
    Создаёт структуру НС
  '''

  model = Sequential()
  model.add(BatchNormalization(input_shape=(x_train.shape[1],)))
  model.add(Dropout(0.2))
  model.add(Dense(128, activation='relu'))
  model.add(Dense(16384, activation='tanh'))
  model.add(Dense(64, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(1, activation='sigmoid'))


  model.summary()
  return model


# Обучаем модель полученными данными
def fitNN_Calls_n(model, x_train, y_train, x_test, y_test):
  '''
    Обучение НС
  '''
  model.compile(optimizer=Adam(learning_rate=1e-6), loss='mse', metrics=['mae'])

  history = model.fit(x_train, 
                      y_train, 
                      epochs=1000, 
                      batch_size=100,
                      validation_data=(x_test, y_test))

  plt.plot(history.history['mae'], 
          label='Средняя абсолютная ошибка на обучающем наборе')
  plt.plot(history.history['val_mae'], 
          label='Средняя абсолютная ошибка на проверочном наборе')
  plt.xlabel('Эпоха обучения')
  plt.ylabel('Средняя абсолютная ошибка')
  plt.legend()
  plt.show()

"""## Обучение.

### Создать заново все модели
"""

# Создать заново всё модели
ns['AllCalls'] = createNN_AllCalls()
ns['calls(0)'] = createNN_Calls_n()
ns['calls(-2)'] = createNN_Calls_n()
ns['calls(-4)'] = createNN_Calls_n()
ns['calls(-7)'] = createNN_Calls_n()

"""### AllCalls"""

ns_name = 'AllCalls'                                                                              # Название НС
ns[ns_name] = createNN_AllCalls()                                                                 # Создать заново НС
fitNN_AllCalls(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name])         # Обучить НС

"""### Calls(0)"""

ns_name = 'calls(0)'                                                                              # Название НС
ns[ns_name] = createNN_Calls_n()                                                                  # Создать заново НС
fitNN_Calls_n(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name])          # Обучить НС

"""### Calls(-2)"""

ns_name = 'calls(-2)'                                                                             # Название НС
ns[ns_name] = createNN_Calls_n()                                                                  # Создать заново НС
fitNN_Calls_n(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name])          # Обучить НС

"""### Calls(-4)"""

ns_name = 'calls(-4)'                                                                             # Название НС
ns[ns_name] = createNN_Calls_n()                                                                  # Создать заново НС
fitNN_Calls_n(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name])          # Обучить НС

"""### Calls(-7)"""

ns_name = 'calls(-7)'                                                                             # Название НС
ns[ns_name] = createNN_Calls_n()                                                                  # Создать заново НС
fitNN_Calls_n(ns[ns_name], x_train, y_train_norm[ns_name], x_test, y_test_norm[ns_name])          # Обучить НС

"""## Сохраняем НС вместе с весами"""

# Список имён моделей
names_model = list(ns.keys())

# Сохраняем список имён моделей
save_dict(path_main + 'models/name_models.pkl', names_model)

# Сохраняем модели
for name_model in names_model:
    ns[name_model].save(path_main + 'models/' + name_model + '.h5')

"""## Загрузить НС вместе с весами"""

# Получить список имён моделей
names_model = load_dict(path_main + 'models/name_models.pkl')

# Загружаем модели
ns = {}
for name_model in names_model:
    ns[name_model] = load_model(path_main + 'models/' + name_model + '.h5')

"""## Проверка результата MAE

### Функции проверки
"""

# словарь абсолютных средних ошибок
mae = {}

def calc_MAE_error_model(ns_name, select='test'):
  '''
    Подсчёт MAE и ошибки 
    select - 'test', 'train'
  '''
  model = ns[ns_name]
  scaler = scalersY[ns_name]

  if select == 'test':
    x = x_test
    y = y_test[ns_name]
  elif select == 'train':
    x = x_train
    y = y_train[ns_name]

  predict_norm = model.predict(x)                                       # результат в нормализованном виде
  predict = scaler.inverse_transform(predict_norm).astype('float64')    # преобразование нормализованных выходных данных к нормальному виду
  

  err = abs(y - predict)                                                # считаем абсолютную ошибку

  mae = round(err.mean(), 2)                                            # считаем MAE - среднюю абсолютную ошибку

  return predict, err, mae 


def get_tables_result(ns_names):
  '''
    Возвращает таблицы с результатами подсчёта MAE для выбранных НС
  '''
  # таблица для MAE
  df_mae = pd.DataFrame(data=ns_names, columns=['ns_name'])
  df_mae['MAE_test'] = ''
  df_mae['MAE_train'] = ''

  # длина таблицы по максимальным данным
  num_row = max([len(y_test[columnsY[0]]), len(y_train[columnsY[0]])])
  
  # таблица для err с кол-вом строк num_row
  df_err = pd.DataFrame(index=[i for i in range(num_row)])

  for ns_name in ns_names:
    for select in ['test', 'train']:
      predict, err, mae = calc_MAE_error_model(ns_name, select)
      name_col = ns_name + '_' + select + '_'

      # дополнить нулями если нужно
      num_add_row = num_row - len(predict)
      predict = np.append(predict, np.zeros((num_add_row)))
      err = np.append(err, np.zeros((num_add_row)))

      predict = np.round(predict, 1)
      err = np.round(err, 1)
     
      # Формируем таблицу
      if select == 'test':
        df_err[name_col + 'true'] = np.round(np.append(y_test[ns_name], np.zeros((num_add_row))), 0)
        df_mae.loc[df_mae.ns_name == ns_name, 'MAE_test'] = mae
      elif select == 'train':
        df_err[name_col + 'true'] = np.round(np.append(y_train[ns_name], np.zeros((num_add_row))), 0)
        df_mae.loc[df_mae.ns_name == ns_name, 'MAE_train'] = mae

      df_err[name_col + 'pred'] = predict
      df_err[name_col + 'err'] = err

  return df_mae, df_err

"""### Ошибка и MAE"""

# Получаем результат
df_mae, df_err = get_tables_result(columnsY)

# Таблица ошибок по разным НС
'''
  AllCalls      - имя НС
  test, train   - проверочная или тренировочная выборки
  true          - табличное (правильное)  значение
  pred          - передсказанное значение
  err           - абсолютная ошибка
'''
df_err.head(25)

# Таблица MAE
'''
  # MAE_test - на проверочной выборке
  # MAE_train - на обучающей выборке
'''
df_mae

"""### Вывести результат по одной НС"""

# Выберите имя НС через индекс массива (0-4)
num = 0
ns_name = ['AllCalls', 'calls(0)', 'calls(-2)', 'calls(-4)', 'calls(-7)']

df_err[df_err.columns[df_err.columns.str.find(ns_name[num]) == 0]].head(50)