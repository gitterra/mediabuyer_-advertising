# -*- coding: utf-8 -*-
"""AleksandrT_ML_v2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d_U_axFl2Vs5KLBpfQTkQaehCdo7518P
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np 

from tensorflow.keras import utils 
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split  # Для разделения выборки

import matplotlib.pyplot as plt  # Для построения и отрисовки графиков
# %matplotlib inline

import joblib # для сохранения MinMaxScaler
import pickle # для записи словаря в файл в формате JSON

from google.colab import drive  # Подключаем google диск
drive.mount('/content/drive')

path_main = '/content/drive/MyDrive/Стажировка/ML_v2/'

"""##Сохранить|загрузить данные"""

def save_data(path, data):
  """
  Сохраняет словарь в файл
  path - путь к файлу
  data - сохраняемые данные (словарь, список и т.д.)

  Описание https://pythonworld.ru/moduli/modul-pickle.html
  """

  with open(path, 'wb') as f:
    pickle.dump(data, f)

  
def load_data(path):
  """
  Загружает словарь из файла
  path - путь к файлу

  Возвращает:
  data - загруженные данные (словарь, список и т.д.)

  Описание https://pythonworld.ru/moduli/modul-pickle.html
  """

  with open(path, 'rb') as f:
    return pickle.load(f)

"""##Сохранить|загрузить словарь"""

def save_dict(path, data_dict):
  """
  Сохраняет словарь в файл
  path - путь к файлу
  data_dict - словарь
  """

  save_data(path, data_dict)

  
def load_dict(path):
  """
  Загружает словарь из файла
  path - путь к файлу

  Возвращает:
  data_dict - словарь
  """

  data_dict = load_data(path)

  return data_dict

"""## Сохранить|загрузить маску"""

def save_masks(path, mask_train, mask_test):
  """
  Сохраняет словарь в файл
  path - путь к файлу
  mask_train - маска тренировочная
  mask_test - маска проверочная
  """
  mask = {'mask_train' : mask_train,
          'mask_test' : mask_test}

  save_data(path, mask)

  
def load_masks(path):
  """
  Загружает словарь из файла
  path - путь к файлу

  Возвращает:
  mask_train - маска тренировочная
  mask_test - маска проверочная
  """

  mask = load_data(path)

  return mask['mask_train'], mask['mask_test']

"""# Подготовка БД"""

# датасет на 3013 записей
df_db = pd.read_csv('/content/drive/MyDrive/Стажировка/Копия линейки со звонками - РС со звонками за 10мин.csv')
df_db

"""##Оставляем только нужное из базы"""

# Имена колонок с входными данными
#columnsX = ['advertiser', 'product', 'TVChannel', 'spread', 'spotLine', 'spotDay', 'spotDuration']
columnsX = ['advertiser', 'product', 'TVChannel', 'spread', 'genre', 'spotLine', 'spotDay', 'spotDuration']
# Имена колонок с выходными данными
columnsY = ['AllCalls', 'calls(0)', 'calls(-2)', 'calls(-4)', 'calls(-7)']

columns = columnsX + columnsY
df_data = pd.DataFrame(data=df_db[columns], index=None, columns=columns, dtype=None)
df_data['spotDay'] = pd.to_datetime(df_data['spotDay'], errors='coerce') # задаём тип столбцу spotDay

# колонку spotDuration переводим в тип object
df_data['spotDuration'] = df_data['spotDuration'].astype(str)

# Перевод spotLine в формат 00:00:00, чтобы удобно было сортировать
df_data['spotLine'] = pd.to_datetime(df_data['spotLine']) # задаём тип столбцу spotLine
df_data['spotLine'] = df_data['spotLine'].dt.strftime('%H:%M:%S')
print(df_data.dtypes)
df_data

for_predict = df_data[['advertiser', 'product', 'TVChannel', 'spread', 'genre', 'spotLine', 'spotDay', 'spotDuration']]

for_predict[::4].head(40).to_excel("/content/drive/MyDrive/Стажировка/for_predict.xlsx", index=False)

"""## Подчищаем базу от NaN"""

# все пустые (не заполненные) ячейки в колонках типа object меняем на '-'
df_data.fillna(
    value = {'advertiser': '-',
             'product': '-',
             'TVChannel': '-',
             'spread': '-',
             'genre': '-',
             'spotLine': '-',
             'spotDuration': '-'}, 
    inplace=True)

# Проверяем на пустые значения
df_data.isna().sum()

"""## Расширяем базу"""

# Словарь типов тв-программ
genre_map = { 0 : '-',
              1 : 'Кино, сериалы и анимация',
              2 : 'Люди и общество (шоу)',
              3 : 'Документальная программа, сериал',
              4 : 'Новости и политика',
              5 : 'Развлечения',
              6 : 'Игры',
              7 : 'Образование, познавательная программа',
              8 : 'Музыка',
              9 : 'Для всей семьи',     
              10 : 'Кулинария',
              11 : 'Прочее'}

# Словарь соответствий тв-программ типам
genre_mapping = {'-': 0,
              'Документальная программа': 3,
              'Познавательная программа': 7,
              'Анимация': 1,
              'Социально-политическая программа': 4,
              'Новости': 4,
              'Развлекательная программа': 5,
              'Кинофильм': 1,
              'Телесериал': 1,
              'Детская программа': 9,
              'Музыкальная программа': 8,
              'Прочее': 11,
              'Сериал (Сериал "Морские дьяволы. Смерч - 2.")': 1,
              'ЗА ГРАНЬЮ': 2, 
              'Место встречи': 4,
              'Сериал (Сериал "Морские дьяволы. Смерч-3")': 1,
              'Ток-шоу «ДНК»': 2,
              'Сериал (Сериал "МОРСКИЕ ДЬЯВОЛЫ. РУБЕЖИ РОДИНЫ")': 1,
              'ОТРИЦАТЕЛИ БОЛЕЗНЕЙ (Научное расследование Сергея Малоземова)': 2,
              'Сериал (Сериал "Морские дьяволы. Смерч")': 1,
              'Новые русские сенсации': 4,
              'Следствие вели...': 3,
              'Обзор. Чрезвычайное происшествие': 4,
              'Сериал (Сериал "Морские дьяволы. Смерч. Судьбы.")': 1,
              'Сегодня/Сегодня в Москве': 4,
              'Сериал (Сериал "Морские дьяволы. Смерч. Судьбы-2")': 1,
              'Сериал (Сериал "АЛЕКС ЛЮТЫЙ. ДЕЛО ШУЛЬЦА")': 1,
              'Сериал (Сериал "Береговая охрана 2")': 1,
              'Сериал (Сериал "Морские дьяволы 3")': 1,
              'Сериал (Сериал "Морские дьяволы 4")': 1,
              'Док. сериал (Д/ф "Знаки судьбы")': 1,
              'Док. сериал (Д/ф "Гадалка")': 1,
              'Док. сериал (Д/ф "СЛЕПАЯ")': 1,
              'Мистические истории': 1,
              'Док. сериал (Д/ф "Уиджи")': 1,
              'Вернувшиеся': 1,
              'Док. сериал (Д/ф "Врачи")': 1,
              'НОВЫЙ ДЕНЬ': 1,
              'Р/б после мультфильмов до M/ф "Возвращение Домовенка"': 9,
              'Своя игра': 5,
              'Сериал (Сериал "Пять минут тишины. Возвращение")': 1,
              'Сериал (Сериал "Морские дьяволы. Судьбы - 2")': 1,
              'Сериал (Сериал "Ментовские войны - 7")': 1,
              'ЖДИ МЕНЯ': 2,
              'Сериал (Сериал "ПЕРВЫЙ ОТДЕЛ")': 1,
              'Шоу «Маска»': 8,
              'Ты не поверишь!': 2,
              'Сериал (Сериал "ПЕРВЫЙ ОТДЕЛ-2")': 1,
              'Документальный фильм (Д/ф "МОИ УНИВЕРСИТЕТЫ. БУДУЩЕЕ ЗА НАСТОЯЩИМ")': 7,
              'Лотерея "У НАС ВЫИГРЫВАЮТ!"': 6,
              'Сериал (Сериал "ПОРТ")': 1,
              'Страна талантов': 2,
              'Сериал (Сериал "Морские дьяволы. Особое задание")': 1,
              'Сериал (Сериал "АНОНИМНЫЙ ДЕТЕКТИВ")': 1,
              'По следу монстра': 1,
              'Сериал (Сериал "Красная зона")': 1,
              'Сериал (Сериал "Шеф 2")': 1,
              'Сериал (Сериал "Морские дьяволы 5")': 1,
              'Сериал (Сериал "Шеф. Новая жизнь")': 1,
              'Сериал (Сериал "Морские дьяволы. Северные рубежи")': 1,
              'Документальный фильм (Д/ф "Анна")': 1,
              'Секрет на миллион': 2,
              'Сериал (Сериал "Балабол-4")': 1,
              'Сериал (Сериал "По ту сторону смерти")': 1,
              'Сериал (Сериал "Сверхъестественное")': 1,
              'Худ. фильм (Х/ф "Хоббит: Пустошь Смауга")': 1,
              'Сериал (Сериал "Ментовские войны - 10")': 1,
              'Сериал (Сериал "Ментовские войны - 11")': 1,
              'Сегодня (вых)': 4,
              'ШОУМАСКГООН': 2,
              'Документальный фильм (Д/ф "ЧЕРНОМОРСКИЙ ЦУГЦВАНГ. ГИБЕЛЬ ТЕПЛОХОДА «АРМЕНИЯ")': 3,
              'Сериал (Сериал "МОРСКИЕ ДЬЯВОЛЫ. ДАЛЬНИЕ РУБЕЖИ")': 1,
              'Док. сериал (Д/ф "Старец")': 1,
              'Добрый день с Валерией': 2,
              'Р/б после мультфильмов до M/ф "В тридесятом веке"': 9,
              'Худ.фильм (Х/ф "Хоббит: Нежданное путешествие")': 1,
              'Худ.фильм (Х/ф "Хоббит: Пустошь Смауга")': 1,
              'Худ. фильм (Х/ф "Хоббит. Битва пяти воинств")': 1,
              'Худ.фильм (Х/ф "Последний легион")': 1,
              'Худ.фильм (Х/ф "Белоснежка: месть гномов")': 1,
              'Худ. фильм (Х/ф "Царство небесное")': 1,
              'Р/б после мультфильмов до M/ф "Влюбчивая ворона"': 9,
              'Звёзды сошлись': 2,
              'Готовим с Алексеем Зиминым': 10,
              'Сериал (Сериал "Невский. Проверка на прочность")': 1,
              'Сериал (Сериал "Проверка на прочность")': 1,
              'Сериал (Сериал "Невский. Чужой среди чужих")': 1,
              'Сериал (Сериал "ГОРЯЧАЯ ТОЧКА-2")': 1,
              'НОВОГОДНЯЯ МАСКА 2022': 2,
              'Сериал (Сериал "Невский")': 1,
              'ЧП.Расследование': 4,
              'Художественный фильм (Х/ф "Чёрный пёс")': 1,
              'Художественный фильм (Х/ф "БАТАЛЬОН")': 1,
              'Сегодня': 4,
              'Сериал (Сериал "Чингачгук")': 1,
              'ОСНОВАНО НА РЕАЛЬНЫХ СОБЫТИЯХ': 4,
              'Сериал (Сериал "Дельфин")': 1,
              'НЕВЕДОМЫЕ ЧУДОВИЩА НА ЗЕМЛЕ (Научное расследование Сергея Малоземова)': 2,
              'Сериал (Сериал "ПОЛИЦЕЙСКОЕ БРАТСТВО")': 1,
              'Сериал (Сериал "ПРАКТИКАНТ 2")': 1,
              'Худ. фильм (Х/ф "Заложница 3")': 1,
              'Худ. фильм (Х/ф "ШУТКИ В СТОРОНУ-2. МИССИЯ В МАЙАМИ")': 1,
              'Художественный фильм (Х/ф "Отставник. Один за всех")': 1,
              'Художественный фильм (Х/ф "Отставник. Спасти врага")': 1,
              'Художественный фильм (Х/ф "ДИНА И ДОБЕРМАН")': 1,
              'Пресс-конференция по итогам встречи министров иностранных дел РФ и Украины': 4,
              '"ТАЙНЫЕ РЕЦЕПТЫ НЕОФИЦИАЛЬНОЙ МЕДИЦИНЫ". Научное расследование Сергея Малозёмова': 2,
              'ТЫ СУПЕР! 60+': 2,
              'Сериал (Сериал "Вспышка")': 1,
              'Сериал (Сериал "Заповедный спецназ")': 1,
              '«ЧТО МОГУТ ЭКСТРАСЕНСЫ?». НАУЧНОЕ РАССЛЕДОВАНИЕ СЕРГЕЯ МАЛОЗЁМОВА': 2}

# Словарь с данными для преобразования колонок
genres_dict = {'genre_map' : genre_map,
               'genre_mapping' : genre_mapping}

# Сохраняем полученный словарь в файл dict_genres.pkl
save_dict(path_main + 'dicts/dict_genres.pkl', genres_dict)

def expand_data(df_data):
    """
    Расширяем входные данные
    Добавляем колонки: spotMonth, weekDay, weekEnd, 
    genre_class (на остове словарей genre_map и genre_mapping)
    :return: Расширенный датафрейм
    """

    # Получаем значения месяцев 1-12
    df_data['spotMonth'] = df_data['spotDay'].dt.month
    # Получаем значения дней недели 0-6
    df_data['weekDay'] = df_data['spotDay'].dt.weekday

    # Получаем значения рабочий/выходной 0
    arr = []
    for day in df_data['spotDay'].dt.weekday:
      if day > 4:
        arr += [1]  # рабочий - 1
      elif np.isnan(day):
        arr += [None]  # дата не определена - NaN
      else: 
        arr += [0]  # выходной - 0
    df_data['weekEnd'] = arr

    # Добавляем колонку genre_class
    arr = []
    for genre in df_data['genre']:
      genre_map = genres_dict['genre_map']
      genre_mapping = genres_dict['genre_mapping'] 
      arr += [genre_map[genre_mapping[genre]]]
    df_data.loc[:,'genre_class'] = arr

    return df_data

df_data = expand_data(df_data)

# Имена колонок с расширенными входными данными
columnsX_plus = columnsX + ['spotMonth', 'weekDay', 'weekEnd']

print(df_data.dtypes)
df_data

"""## Анализирум выходные данные

### Гистограмма
"""

# Задаем пространство для подграфиков
f, axes = plt.subplots(5, 1, figsize=(25,15))

# Гистограмма распределения значений в массиве
for i, col_name in enumerate(columnsY):
  color = 'g' 
  if (i == 0): color = 'tab:orange'
  axes[i].hist(df_data[col_name].values, bins=100, color=color) # параметр bins отвечает за количество подгрупп, в которые объединяются данные 
  axes[i].set_ylabel(col_name)

plt.show()

"""### Выбор максимальных значений

Т.к. большая часть выходных значений в БД ближе к нулю, то выберем максимально допустимые значения для колонок 

columnsY = ['AllCalls', 'calls(0)', 'calls(-2)', 'calls(-4)', 'calls(-7)']
"""

# Выберем максимальные значения для выходных данных
columnsY_maxval = {'AllCalls': 200, 
                    'calls(0)': 100, 
                    'calls(-2)': 30, 
                    'calls(-4)': 40, 
                    'calls(-7)': 20}

"""## Обрезаем выходные значения

Обрежем значения в колонках

columnsY = ['AllCalls', 'calls(0)', 'calls(-2)', 'calls(-4)', 'calls(-7)']

которые выше значений в указанных в словаре columnsY_maxval для колонок.

"""

def set_max_dataY(df_data, columnsY, columnsY_maxval):
  """
  Обрезаем масимальные значения выходных данных в колонках columnsY у df_data
  по значениям в columnsY_maxval
  """
  
  # Цикл по выходным колонкам
  for col_name in columnsY:
    df_col = df_data[col_name].copy()                                           # Копируем колонку
    max_val = columnsY_maxval[col_name]                                         # Определяем максимальное значение
    mask = df_col[:] > max_val                                                  # Находим строки, где значения > заданных в columnsY_maxval
    df_col.loc[mask] = max_val                                                  # Присваиваем максимальные значения из columnsY_maxval
    df_data.loc[:, col_name] = df_col                                           # Сохраняем значения в датафрейме


# Обрезаем выходные значения
set_max_dataY(df_data, columnsY, columnsY_maxval)

# Проверяем результат
df_data.describe()

"""## Парсим базу

### dataX, dataY, dataX_dict
"""

def words_to_OHE(df_col):
  '''
    Входной массив преобразуется в OHE
    Размер OHE определяется автоматически

    df_col = ['-', 'РД1', 'РД2', 'РД1', 'РД3', 'РД3', 'РД4', 'РД4']
    words_dict = {'-': 0, 'РД1': 1, 'РД2': 2, 'РД3': 3, 'РД4': 4}
    words_OHE =   array([[0., 0., 0., 0.],    '-' - отбрасывается при приведении к OHE
                         [1., 0., 0., 0.],
                         [0., 1., 0., 0.],
                         [1., 0., 0., 0.],
                         [0., 0., 1., 0.],
                         [0., 0., 1., 0.],
                         [0., 0., 0., 1.],
                         [0., 0., 0., 1.]], dtype=float32)
  
    Все строковые значения, которые не указаны в таблице, значаться как '-'
  '''

  # Удаляем '-' из массива уникальных значений
  unique = df_col.unique()
  if '-' in unique:
    unique = list(unique)
    unique.remove('-')
    unique = np.array(unique)

  # Получаем словарь с уникальными значениями
  words_dict = {j: i+1 for i, j in enumerate(unique)}
  # Принудительно назначаем {'-': 0}
  words_dict['-'] = 0

  # Получаем OHE, где перый элемент отвечает за '-'
  words_OHE = np.array(utils.to_categorical([words_dict[i] for i in df_col.values], num_classes=len(words_dict)))

  # Удаляем первый элемент из OHE. Тогда везде, где встречается '-' будет OHE с нулями [0, 0 ... 0, 0]
  words_OHE = words_OHE[:, 1:]

  #print(words_dict)
  #print(words_OHE)

  return words_OHE, words_dict


def getDataX(df_data):
  '''
    Подготовка датасета по входным параметрам
  '''
  dataX_dict = {}
  advertiser_OHE, dataX_dict['advertiser_dict'] = words_to_OHE(df_data.advertiser)
  product_OHE, dataX_dict['product_dict'] = words_to_OHE(df_data['product'])
  TVChannel_OHE, dataX_dict['TVChannel_dict'] = words_to_OHE(df_data.TVChannel)
  spread_OHE, dataX_dict['spread_dict'] = words_to_OHE(df_data.spread)
  _ , dataX_dict['genre_dict'] = words_to_OHE(df_data.genre)
  genre_class_OHE, dataX_dict['genre_class_dict'] = words_to_OHE(df_data.genre_class)
  spotLine_OHE, dataX_dict['spotLine_dict'] = words_to_OHE(df_data.spotLine)
  spotDuration_OHE, dataX_dict['spotDuration_dict'] = words_to_OHE(df_data.spotDuration)

  # все пустые (не заполненные) ячейки в колонке spotMonth меняем на 13
  df_data.fillna(
      value = {'spotMonth': 13}, 
      inplace=True)
  spotMonth_OHE = utils.to_categorical(df_data['spotMonth'].values - 1, num_classes=13)
  # Удаляем последний элемент из OHE. Тогда везде, где встречается 'NaN' будет OHE с нулями [0, 0 ... 0, 0]
  spotMonth_OHE = spotMonth_OHE[:, :-1]

  # все пустые (не заполненные) ячейки в колонке weekDay меняем на 7
  df_data.fillna(
      value = {'weekDay': 7}, 
      inplace=True)
  weekDay_OHE = utils.to_categorical(df_data['weekDay'].values, num_classes=8)
  # Удаляем последний элемент из OHE. Тогда везде, где встречается 'NaN' будет OHE с нулями [0, 0 ... 0, 0]
  weekDay_OHE = weekDay_OHE[:, :-1]

  # все пустые (не заполненные) ячейки в колонке weekEnd меняем на 2
  df_data.fillna(
      value = {'weekEnd': 2}, 
      inplace=True)
  weekEnd_OHE = utils.to_categorical(df_data['weekEnd'].values, num_classes=3)
  weekEnd_OHE = weekEnd_OHE[:, :-1]

  # Объединяем все OHE в один вектор
  dataX_OHE = np.hstack([advertiser_OHE, product_OHE, TVChannel_OHE, spread_OHE, genre_class_OHE, spotLine_OHE, spotDuration_OHE, spotMonth_OHE, weekDay_OHE, weekEnd_OHE])

  print('dataX_OHE:')
  print(dataX_OHE)
  for x in dataX_dict:
    print(x + ': ', end='')
    print(dataX_dict[x])

  return dataX_OHE, dataX_dict


def getDataY(df_data):
  '''
    Подготовка датасета по выходным параметрам
  '''
  dataY = {}

  for name_col in columnsY:
    dataY[name_col] = df_data[name_col].values.reshape(-1,1)

  print('dataY:')
  print(dataY)
  return dataY

# Получаем dataX в OHE
dataX, dataX_dict = getDataX(df_data)
# Получаем dataY в обычном виде
dataY = getDataY(df_data)

# Сохраняем полученный словарь в файл dictX.pkl
save_dict(path_main + 'dicts/dictX.pkl', dataX_dict)

"""### dataX_for_site_dict"""

"""
  Формируем данные в виде списка для заполнения входных полей на сайте
  [{'name': 'TVChannel':         - название поля
    'type': 'string',            - тип поля (object - string, datetime64 - date, int64 - number)
    'num_classes': 10,           - кол-во классов (длина словаря values)
    'values': {'-': 0,           - словарь со значениями полей и номером класса для OHE
              'ГНЦ': 1,
              'НЦЗ': 2}}, ...]
"""
dataX_for_site = []
for name in columnsX:
  field = {'name': name}                    # название поля

  type_field = '-'
  values = {}
  if df_data[name].dtype == 'O':
    type_field = 'string'
    values = dataX_dict[name + '_dict']
  elif df_data[name].dtype == 'int64':
    type_field = 'number'
  elif df_data[name].dtype == '<M8[ns]':
    type_field = 'date'
  field['type'] = type_field                # тип поля (object - string, datetime64 - date, int64 - number)

  num_classes = len(values)                 

  field['num_classes'] = num_classes        # кол-во классов (длина словаря values)

  values = dict(sorted(values.items()))     # сортировка
  field['values'] = values                  # словарь со значениями полей и номером класса для OHE

  dataX_for_site += [field]                 # добавляем в список

# Сохраняем полученный словарь в файл dataX_for_site.pkl
save_dict(path_main + 'dicts/dataX_for_site.pkl', dataX_for_site)

# пример
dataX_for_site[5:8]

"""## Обучающая и тестовая выборки"""

print('Размерность входных данных: ' + str(dataX.shape))
for n in dataY:
  print('Размерность выходных данных ' + n +': ' + str(dataY[n].shape))

# Пример входных и выходных данных
print(dataX[0])
print(dataY['AllCalls'][0])

"""### Маска train и test"""

# Делим датасет на обучающую и проверочную выборки
# Длина датасета
len_db = len(dataX)                                                         
print('Длина датасета: ', len_db)

# Маски для тренировочной и проверочной выборок
mask_train, mask_test = train_test_split(np.arange(len_db), test_size = 0.1)

print('Длина тренировочной выборки: ', len(mask_train))
print('Длина проверочной выборки: ', len(mask_test))

# Сохраняем полученные маски в файл masks.pkl
save_masks(path_main + 'masks/masks.pkl', mask_train, mask_test)

# Загружаем маски из файла masks.pkl
mask_train, mask_test = load_masks(path_main + 'masks/masks.pkl')

"""### x_train, x_test, y_train, y_test"""

def get_train_test_split(dataX, dataY, mask_train, mask_test, columnsY):
  '''
    Делит датасет на обучающую и проверочную выборки так,
    чтобы для всех выходных данных в dataY значения были из одной строки
  '''
  y_train, y_test = {}, {}

  # Разбиваем на тренировочную и проверочную для dataX
  x_train = dataX[mask_train]
  x_test = dataX[mask_test]

  # Разбиваем на тренировочную и проверочную для dataY
  for col in columnsY:
    y_train[col] = dataY[col][mask_train]
    y_test[col] = dataY[col][mask_test]
 
  return x_train, x_test, y_train, y_test

# Делим датасет на обучающую и проверочную выборки
x_train, x_test, y_train, y_test = get_train_test_split(dataX, dataY, mask_train, mask_test, columnsY)

print('Входные данные')
print(x_train.shape)
print(x_test.shape)
print()
for col_name in columnsY:
  print('Колонка ', col_name)
  print(y_train[col_name].shape)
  print(y_test[col_name].shape)
  print()

"""## Нормализация"""

# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.inverse_transform

def getScalersY(dataY):
  '''
    Создание словаря скеллеров для выходных данных
  '''
  scalersY = {} # список скэйлеров

  for name_col in columnsY:
    scalersY[name_col] = MinMaxScaler()
    scalersY[name_col].fit(dataY[name_col])

    # Сохраняем MinMaxScaler
    path = path_main + 'scalers/' + name_col + '.pkl'
    save_data(path, scalersY[name_col])
    print('Скелер сохранен: ' + path)

  # Сохранить список имён скалеров
  path = path_main + 'scalers/columnsY.pkl'
  save_dict(path, columnsY)
  print('Список скелеров сохранен: ' + path)

  return scalersY


def get_normY(y):
  '''
    Нормализация выходных данных
  '''
  y_norm = {}

  # Загрузить список имён скалеров
  path = path_main + 'scalers/columnsY.pkl'
  columnsY = load_dict(path)
  print('Список скелеров загружен: ' + path)

  for name_col in columnsY:
    # Загружаем скалеры из файлов
    path = path_main + 'scalers/' + name_col + '.pkl'
    scalersY[name_col] = load_data(path)
    print('Скелер загружен: ' + path)

    # Нормализуем данные с помощью скелеров 
    y_norm[name_col] = scalersY[name_col].transform(y[name_col])

  return y_norm

# Создаём скеллеры для выходных данных
scalersY = getScalersY(dataY)

# Норамализуем y_train, y_test
y_train_norm_dict = get_normY(y_train)
y_test_norm_dict = get_normY(y_test)

print('x:')
print(x_train.shape)
print(x_test.shape)
print('y:')
print(y_train['AllCalls'].shape)
print(y_test['AllCalls'].shape)
print('y_norm:')
print(y_train_norm_dict['AllCalls'].shape)
print(y_test_norm_dict['AllCalls'].shape)

# пример норминованных данных
print('Нормированные выходные данные')
print(y_train_norm_dict[columnsY[1]])
# пример преобразования нормализованных выходных данных к нормальному виду
print('НЕ нормированные выходные данные')
print(scalersY[columnsY[1]].inverse_transform(y_train_norm_dict[columnsY[1]]))

"""#Модель

Необходимые функции
"""

# импортируем модуль метрик
from sklearn import metrics

# Функция подсчета MAE
def mae(model, name_col):
  pred = model.predict(x_test)
  pred = pred.reshape(-1,1)
  predUnscaled = scalersY[name_col].inverse_transform(pred)
  y_testUnscaled = scalersY[name_col].inverse_transform(y_test_norm_dict[name_col])
  delta = predUnscaled - y_testUnscaled
  absDelta = abs(delta)
  print('MAE:', sum(absDelta) / len(absDelta))

# Функция обучения и проверки моделей
def experiments(ns_dict, y_train_norm_dict, save = 0):

  # проходим по y_train_norm каждой орбиты
  for y_orbyt in y_train_norm_dict:
    print(f'Орбита - {y_orbyt}')

    # создадим объект этого класса и запишем в переменную model
    model_ML = ns_dict[y_orbyt]
    # обучим нашу модель  
    # .ravel() используется для того чтобы функция подошла для RandomForestRegressor
    model_ML.fit(x_train, y_train_norm_dict[y_orbyt].ravel())

    # Делаем предсказание и приводим его к начальному виду
    pred = model_ML.predict(x_test)
    pred = pred.reshape(-1,1)
    predUnscaled = scalersY[y_orbyt].inverse_transform(pred)
    y_testUnscaled = scalersY[y_orbyt].inverse_transform(y_test_norm_dict[y_orbyt])
    # Для подсчета MAE используем модуль metrics из библиотеки sklearn
    print('mean_absolute_error (MAE):', metrics.mean_absolute_error(y_testUnscaled, predUnscaled))

    if save:
      # Путь к файлу с моделью
      filename = path_main + f'models/{y_orbyt}.sav'
      # Сохранение
      save_data(filename, model_ML)
      #pickle.dump(model_ML, open(filename, 'wb'))
      print(f'Модель {y_orbyt} сохранена')

"""##LinearRegression"""

# из набора линейных моделей библиотеки sklearn импортируем линейную регрессию
from sklearn.linear_model import LinearRegression

# Создаем словарь с моделями
ns_dict = {}
for col in columnsY:
  ns_dict[col] = LinearRegression()
  print(f'Модель для {col} создана, id - {id(ns_dict[col])}' )

# Обучаем и проверяем модели
experiments(ns_dict, y_train_norm_dict)

"""##DecisionTreeRegressor"""

from sklearn.tree import DecisionTreeRegressor

# Создаем словарь с моделями
ns_dict = {}
for col in columnsY:
  ns_dict[col] = DecisionTreeRegressor()
  print(f'Модель для {col} создана, id - {id(ns_dict[col])}' )

# Обучаем и проверяем модели
experiments(ns_dict, y_train_norm_dict)

"""##RandomForestRegressor"""

from sklearn.ensemble import RandomForestRegressor

# Создаем словарь с моделями
ns_dict = {}
for col in columnsY:
  ns_dict[col] = RandomForestRegressor()
  print(f'Модель для {col} создана, id - {id(ns_dict[col])}' )

# Обучаем и проверяем модели
experiments(ns_dict, y_train_norm_dict, save = 1)

"""Сохранение списка имен моделей"""

# Список имён моделей
names_model = list(ns_dict.keys())

# Сохраняем список имён моделей
save_dict(path_main + 'models/name_models.pkl', names_model)